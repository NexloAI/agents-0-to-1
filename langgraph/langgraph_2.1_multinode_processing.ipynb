{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "vscode": {
          "languageId": "raw"
        }
      },
      "source": [
        "\n",
        "# LangGraph Unit 2 Tutorial: Advanced State Management and Multi-Node Processing\n",
        "\n",
        "## Introduction to Advanced LangGraph Patterns\n",
        "\n",
        "Building on our foundation from Unit 1, Unit 2 takes us deeper into sophisticated state management and multi-node processing patterns. This unit represents the transition from basic conversational agents to production-ready systems that can handle complex workflows, manage memory efficiently, and process information through coordinated pipelines.\n",
        "\n",
        "### What Makes Unit 2 Special:\n",
        "- **Advanced State Management:** Go beyond simple message tracking to include summaries, window controls, and custom fields\n",
        "- **Memory Optimization:** Implement sliding windows and dynamic summarization for long-running conversations\n",
        "- **Multi-Node Pipelines:** Build sophisticated processing chains with specialized nodes working in harmony\n",
        "- **Production Patterns:** Learn patterns used in real-world applications\n",
        "\n",
        "This tutorial consolidates Unit 12 exercises from AI Product Engineer, covering:\n",
        "1. **Advanced State Management** - Extended fields for context tracking, adding summary and windows size\n",
        "2. **Message Processing with State Preservation** - Sophisticated message handling\n",
        "3. **Sliding Window Memory Management** - Efficient conversation history\n",
        "4. **Dynamic Conversation Summarization** - Context preservation through summaries\n",
        "5. **Multi-Node Processing Pipelines** - Complex workflow orchestration\n",
        "\n",
        "### Key Learning Outcomes:\n",
        "By the end of this tutorial, you'll be able to:\n",
        "- Design sophisticated state structures for complex applications\n",
        "- Implement memory-efficient conversation management\n",
        "- Build multi-stage processing pipelines"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "vscode": {
          "languageId": "raw"
        }
      },
      "source": [
        "## Setting Up Our Environment\n",
        "\n",
        "Before we dive into advanced patterns, let's ensure our environment is properly configured with all necessary dependencies.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Note: you may need to restart the kernel to use updated packages.\n"
          ]
        }
      ],
      "source": [
        "# Install required packages\n",
        "%pip install langgraph langchain langchain-openai python-dotenv --quiet\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {},
      "outputs": [],
      "source": [
        "import os\n",
        "from typing import Annotated, TypedDict, List\n",
        "from dotenv import load_dotenv\n",
        "\n",
        "# Load environment variables\n",
        "load_dotenv()\n",
        "\n",
        "# Set API keys\n",
        "os.environ[\"OPENAI_API_KEY\"] = os.getenv('OPENAI_API_KEY')\n",
        "\n",
        "# Import core dependencies\n",
        "from langchain_core.messages import BaseMessage, HumanMessage, AIMessage, SystemMessage\n",
        "from langgraph.graph import StateGraph, START, END\n",
        "from langgraph.graph.message import add_messages\n",
        "from langchain_openai import ChatOpenAI\n",
        "from langchain.prompts import PromptTemplate\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "vscode": {
          "languageId": "raw"
        }
      },
      "source": [
        "## Exercise 1: Advanced State Management with Extended Fields\n",
        "\n",
        "Let's start by exploring how to extend our state beyond simple message tracking. This pattern is fundamental for building sophisticated conversational agents that need to maintain various types of context.\n",
        "\n",
        "### Key Concepts:\n",
        "- **Extended State Fields**: State can track more than just messages\n",
        "- **Context Management**: Additional fields enable sophisticated tracking\n",
        "- **Memory Control**: Window size parameter enables memory management\n",
        "- **Type Safety**: TypedDict ensures proper field typing\n",
        "\n",
        "### Why This Matters:\n",
        "In production applications, you often need to track:\n",
        "- Conversation summaries for context retention\n",
        "- User preferences and settings\n",
        "- Processing metadata and analytics\n",
        "- Configuration parameters\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 19,
      "metadata": {},
      "outputs": [],
      "source": [
        "class State(TypedDict):\n",
        "    \"\"\"\n",
        "    Enhanced state container with context management capabilities.\n",
        "    \n",
        "    This implementation demonstrates advanced state management by tracking:\n",
        "    1. Message history with proper LangGraph annotations\n",
        "    2. Conversation summaries for context retention\n",
        "    3. Memory window control for efficient processing\n",
        "    \n",
        "    Attributes:\n",
        "        messages: List of conversation messages with LangGraph's add_messages\n",
        "                 annotation for proper message handling\n",
        "        summary: Running summary of the conversation context\n",
        "        window_size: Control parameter for message history retention\n",
        "        \n",
        "    Note:\n",
        "        The add_messages annotation is crucial for proper message\n",
        "        handling in LangGraph, ensuring correct state updates while\n",
        "        the additional fields enable sophisticated conversation management.\n",
        "    \"\"\"\n",
        "    messages: Annotated[list[BaseMessage], add_messages]\n",
        "    summary: str\n",
        "    window_size: int\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 20,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Advanced State Management Example:\n",
            "--------------------------------------------------\n",
            "Messages: 2\n",
            "Summary: User inquiring about LangGraph framework\n",
            "Window Size: 3\n"
          ]
        }
      ],
      "source": [
        "# Example: Working with Advanced State\n",
        "print(\"Advanced State Management Example:\")\n",
        "print(\"-\" * 50)\n",
        "\n",
        "# Initialize state with all fields\n",
        "state: State = {\n",
        "    \"messages\": [],\n",
        "    \"summary\": \"\",\n",
        "    \"window_size\": 3\n",
        "}\n",
        "\n",
        "# Add messages\n",
        "state[\"messages\"].append(HumanMessage(content=\"Tell me about LangGraph\"))\n",
        "state[\"messages\"].append(AIMessage(content=\"LangGraph is a powerful framework...\"))\n",
        "\n",
        "# Update summary\n",
        "state[\"summary\"] = \"User inquiring about LangGraph framework\"\n",
        "\n",
        "# Display state\n",
        "print(f\"Messages: {len(state['messages'])}\")\n",
        "print(f\"Summary: {state['summary']}\")\n",
        "print(f\"Window Size: {state['window_size']}\")\n",
        "\n",
        "# Demonstrate window management\n",
        "if len(state[\"messages\"]) > state[\"window_size\"]:\n",
        "    # Keep only the last N messages\n",
        "    state[\"messages\"] = state[\"messages\"][-state[\"window_size\"]:]\n",
        "    print(f\"\\nPruned messages to window size: {state['window_size']}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "vscode": {
          "languageId": "raw"
        }
      },
      "source": [
        "## Exercise 2: Advanced Message Processing with State Preservation\n",
        "\n",
        "Now let's create message processing that demonstrates proper state management and field preservation. This is crucial when building systems that need to maintain consistency across state updates.\n",
        "\n",
        "### Key Concepts:\n",
        "- **State Preservation**: Always maintain all fields during updates\n",
        "- **Conditional Processing**: Different logic based on state content\n",
        "- **Context-Aware Handling**: Use full state context for decisions\n",
        "\n",
        "### Common Pitfalls to Avoid:\n",
        "1. Forgetting to preserve non-message state fields\n",
        "2. Incorrect message type usage\n",
        "3. Missing empty state handling\n",
        "4. Improper annotation usage\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 21,
      "metadata": {},
      "outputs": [],
      "source": [
        "def process_message(state: State) -> State:\n",
        "    \"\"\"Process messages while maintaining conversation context and state.\n",
        "\n",
        "    This processor demonstrates several advanced concepts:\n",
        "    1. Full state preservation across updates\n",
        "    2. Conditional response generation\n",
        "    3. Context-aware message handling\n",
        "\n",
        "    The processing follows this flow:\n",
        "    1. Handle empty state initialization\n",
        "    2. Process existing messages with context\n",
        "    3. Preserve non-message state fields\n",
        "\n",
        "    Args:\n",
        "        state: Current conversation state containing messages,\n",
        "              summary, and configuration\n",
        "\n",
        "    Returns:\n",
        "        State: Updated state with new messages and preserved fields\n",
        "\n",
        "    Example:\n",
        "        >>> initial_state = {\"messages\": [], \"summary\": \"\", \"window_size\": 3}\n",
        "        >>> new_state = process_message(initial_state)\n",
        "        >>> print(new_state[\"messages\"][0].content)\n",
        "        \"Hello!\"\n",
        "    \"\"\"\n",
        "    # Initialize state if empty\n",
        "    if not state[\"messages\"]:\n",
        "        return {\n",
        "            \"messages\": [HumanMessage(content=\"Hello!\")],\n",
        "            \"summary\": \"\",\n",
        "            \"window_size\": 3,\n",
        "        }\n",
        "\n",
        "    # Process last message with context\n",
        "    last_message = state[\"messages\"][-1]\n",
        "    if last_message.content == \"Hello!\":\n",
        "        return {\n",
        "            \"messages\": [HumanMessage(content=\"How are you?\")],\n",
        "            \"summary\": state[\"summary\"],\n",
        "            \"window_size\": state[\"window_size\"],\n",
        "        }\n",
        "\n",
        "    # Maintain state if no conditions met\n",
        "    return state"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 22,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "Testing Message Processing:\n",
            "--------------------------------------------------\n",
            "Initial message: Hello!\n",
            "Summary: \n",
            "Window preserved: 3\n",
            "\n",
            "------------------------------\n",
            "\n",
            "Response to greeting: Hello there!\n",
            "Updated summary: \n"
          ]
        }
      ],
      "source": [
        "# Test message processing with state preservation\n",
        "print(\"\\nTesting Message Processing:\")\n",
        "print(\"-\" * 50)\n",
        "\n",
        "# Test 1: Empty state initialization\n",
        "initial_state = {\"messages\": [], \"summary\": \"\", \"window_size\": 5}\n",
        "result = process_message(initial_state)\n",
        "print(f\"Initial message: {result['messages'][0].content}\")\n",
        "print(f\"Summary: {result['summary']}\")\n",
        "print(f\"Window preserved: {result['window_size']}\")\n",
        "\n",
        "print(\"\\n\" + \"-\" * 30 + \"\\n\")\n",
        "\n",
        "# Test 2: Process greeting\n",
        "greeting_state = {\n",
        "    \"messages\": [HumanMessage(content=\"Hello there!\")],\n",
        "    \"summary\": \"\",\n",
        "    \"window_size\": 3\n",
        "}\n",
        "result = process_message(greeting_state)\n",
        "print(f\"Response to greeting: {result['messages'][-1].content}\")\n",
        "print(f\"Updated summary: {result['summary']}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Debug Tips\n",
        "\n",
        "### Message Processing Issues\n",
        "- Print state before and after processing to verify field preservation\n",
        "- Check message list length matches window_size configuration  \n",
        "- Verify message content types (HumanMessage vs AIMessage)\n",
        "\n",
        "### State Management\n",
        "- Ensure all required fields are present in state updates\n",
        "- Validate summary updates maintain proper format\n",
        "- Monitor window_size compliance\n",
        "\n",
        "### Common Errors\n",
        "- **KeyError**: Usually indicates missing state fields\n",
        "- **TypeError**: Often means incorrect message type usage\n",
        "- **IndexError**: Check empty message list handling\n",
        "\n",
        "## Key Takeaways\n",
        "\n",
        "### State Management\n",
        "- Always preserve all state fields during updates\n",
        "- Use proper type annotations for safety\n",
        "- Maintain consistency in field updates\n",
        "\n",
        "### Message Processing\n",
        "- Handle empty states explicitly\n",
        "- Process messages with full context\n",
        "- Implement clear processing logic\n",
        "\n",
        "## Common Pitfalls\n",
        "- Forgetting to preserve non-message state fields\n",
        "- Incorrect message type usage\n",
        "- Missing empty state handling\n",
        "- Improper annotation usage\n",
        "\n",
        "## Next Steps\n",
        "- Add error handling and recovery\n",
        "- Implement summary generation logic\n",
        "- Add window size enforcement\n",
        "- Integrate with external message processors\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "vscode": {
          "languageId": "raw"
        }
      },
      "source": [
        "## Exercise 3: Message History Management with Sliding Windows\n",
        "\n",
        "Efficient message history management is crucial for production systems. Let's implement a sliding window approach that maintains conversation context while optimizing memory usage.\n",
        "\n",
        "### Why Sliding Windows Matter:\n",
        "- **Limited memory resources** in production environments\n",
        "- **Performance impact** of large message histories\n",
        "- **Relevance decay** of older messages\n",
        "- **Cost optimization** for API calls\n",
        "\n",
        "### Key Implementation Details:\n",
        "- Automatic message pruning when exceeding window size\n",
        "- Preservation of important context through summaries\n",
        "- Efficient state updates\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 24,
      "metadata": {},
      "outputs": [],
      "source": [
        "def message_windowing(state: State) -> State:\n",
        "    \"\"\"Maintain optimal message history through window-based pruning.\n",
        "\n",
        "    This function implements several key concepts:\n",
        "    1. Automatic message pruning\n",
        "    2. Window-based history management\n",
        "    3. State preservation during updates\n",
        "\n",
        "    The windowing process follows this flow:\n",
        "    1. Check current message count\n",
        "    2. Apply window constraints if needed\n",
        "    3. Preserve state consistency\n",
        "\n",
        "    Args:\n",
        "        state: Current conversation state with messages and window configuration\n",
        "\n",
        "    Returns:\n",
        "        State: Updated state with windowed message history\n",
        "\n",
        "    Example:\n",
        "        >>> state = {\n",
        "        ...     \"messages\": [HumanMessage(content=f\"Message {i}\")\n",
        "        ...                 for i in range(5)],\n",
        "        ...     \"summary\": \"\",\n",
        "        ...     \"window_size\": 3\n",
        "        ... }\n",
        "        >>> result = message_windowing(state)\n",
        "        >>> len(result[\"messages\"]) # Should be 3\n",
        "    \"\"\"\n",
        "    if len(state[\"messages\"]) > state[\"window_size\"]:\n",
        "        state[\"messages\"] = state[\"messages\"][-state[\"window_size\"] :]\n",
        "    return state"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 26,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "Sliding Window Management Demo:\n",
            "--------------------------------------------------\n",
            "Initial message count: 8\n",
            "Window size: 3\n",
            "\n",
            "After windowing:\n",
            "Message count: 3\n",
            "Retained messages:\n",
            "  - AIMessage: Answer 5: Here's information about topic 4\n",
            "  - HumanMessage: Question 6: Tell me about topic 6\n",
            "  - AIMessage: Answer 7: Here's information about topic 6\n",
            "\n",
            "Updated summary: Ongoing Q&A session...\n"
          ]
        }
      ],
      "source": [
        "# Demonstrate sliding window management\n",
        "print(\"\\nSliding Window Management Demo:\")\n",
        "print(\"-\" * 50)\n",
        "\n",
        "# Create state with many messages\n",
        "demo_state = {\n",
        "    \"messages\": [\n",
        "        HumanMessage(content=f\"Question {i}: Tell me about topic {i}\")\n",
        "        if i % 2 == 0\n",
        "        else AIMessage(content=f\"Answer {i}: Here's information about topic {i-1}\")\n",
        "        for i in range(8)\n",
        "    ],\n",
        "    \"summary\": \"Ongoing Q&A session\",\n",
        "    \"window_size\": 3\n",
        "}\n",
        "\n",
        "print(f\"Initial message count: {len(demo_state['messages'])}\")\n",
        "print(f\"Window size: {demo_state['window_size']}\")\n",
        "\n",
        "# Apply window management\n",
        "windowed_state = message_windowing(demo_state)\n",
        "\n",
        "print(f\"\\nAfter windowing:\")\n",
        "print(f\"Message count: {len(windowed_state['messages'])}\")\n",
        "print(f\"Retained messages:\")\n",
        "for msg in windowed_state['messages']:\n",
        "    print(f\"  - {type(msg).__name__}: {msg.content}\")\n",
        "print(f\"\\nUpdated summary: {windowed_state['summary'][:100]}...\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "vscode": {
          "languageId": "raw"
        }
      },
      "source": [
        "## Exercise 4: Dynamic Conversation Summarization\n",
        "\n",
        "Maintaining context in long-running conversations requires intelligent summarization. Let's create a system that dynamically generates summaries to preserve important context even when messages are pruned.\n",
        "\n",
        "### Key Benefits:\n",
        "- **Context Preservation**: Keep important information even after pruning\n",
        "- **Memory Efficiency**: Reduce token usage in LLM calls\n",
        "- **Better Responses**: Enable context-aware responses\n",
        "- **Scalability**: Support long-running conversations\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {},
      "outputs": [],
      "source": [
        "def summary_generation(state: State) -> State:\n",
        "    \"\"\"Generate and maintain dynamic conversation summaries.\n",
        "\n",
        "    This function implements several advanced concepts:\n",
        "    1. Threshold-based summary generation\n",
        "    2. Context preservation through summarization\n",
        "    3. State-aware summary updates\n",
        "\n",
        "    The summarization process follows this flow:\n",
        "    1. Check message threshold\n",
        "    2. Process message history\n",
        "    3. Generate contextual summary\n",
        "    4. Update state with new summary\n",
        "\n",
        "    Args:\n",
        "        state: Current conversation state with messages and existing summary\n",
        "\n",
        "    Returns:\n",
        "        State: Updated state with new summary\n",
        "\n",
        "    Example:\n",
        "        >>> messages = [\n",
        "        ...     HumanMessage(content=\"Hello\"),\n",
        "        ...     HumanMessage(content=\"How are you\"),\n",
        "        ...     HumanMessage(content=\"Goodbye\")\n",
        "        ... ]\n",
        "        >>> state = {\"messages\": messages, \"summary\": \"\", \"window_size\": 3}\n",
        "        >>> result = summary_generation(state)\n",
        "        >>> print(result[\"summary\"])\n",
        "        \"Conversation summary: Hello -> How are you -> Goodbye\"\n",
        "    \"\"\"\n",
        "    if len(state[\"messages\"]) > 2:\n",
        "        messages_text = \" -> \".join([m.content for m in state[\"messages\"]])\n",
        "        state[\"summary\"] = f\"Conversation summary: {messages_text}\"\n",
        "    return state"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Debug Tips\n",
        "\n",
        "## Summary Generation Issues:\n",
        "- Log message contents before summarization\n",
        "- Verify summary format consistency  \n",
        "- Check threshold conditions\n",
        "\n",
        "## State Management:\n",
        "- Monitor summary field updates\n",
        "- Validate message processing\n",
        "- Check state preservation\n",
        "\n",
        "## Common Errors:\n",
        "- **AttributeError**: Verify message object structure\n",
        "- **String formatting issues**: Check message content types\n",
        "- **State mutation problems**: Verify proper state handling"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 27,
      "metadata": {},
      "outputs": [],
      "source": [
        "graph = StateGraph(State)\n",
        "graph.add_node(\"summarizer\", summary_generation)\n",
        "graph.add_edge(START, \"summarizer\")\n",
        "\n",
        "\n",
        "generate_dynamic_summary = graph.compile()\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 28,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "image/png": "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",
            "text/plain": [
              "<IPython.core.display.Image object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "from langchain_core.runnables.graph import MermaidDrawMethod\n",
        "from IPython.display import Image, display\n",
        "\n",
        "display(\n",
        "    Image(\n",
        "        generate_dynamic_summary.get_graph().draw_mermaid_png(         \n",
        "            draw_method=MermaidDrawMethod.API               \n",
        "        )\n",
        "    )\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 29,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "Dynamic Summarization Demo:\n",
            "--------------------------------------------------\n",
            "Message count: 6\n",
            "\n",
            "Generated Summary:\n",
            "Conversation summary: What is LangGraph? -> LangGraph is a framework for building stateful applications with LLMs. -> Can it handle multiple agents? -> Yes, LangGraph supports multi-agent workflows and coordination. -> How does state management work? -> State is managed through TypedDict structures with proper annotations.\n",
            "\n",
            "Summary triggered at message count: 6\n"
          ]
        }
      ],
      "source": [
        "# Test dynamic summarization\n",
        "print(\"\\nDynamic Summarization Demo:\")\n",
        "print(\"-\" * 50)\n",
        "\n",
        "# Create a conversation that needs summarization\n",
        "test_conversation = {\n",
        "    \"messages\": [\n",
        "        HumanMessage(content=\"What is LangGraph?\"),\n",
        "        AIMessage(content=\"LangGraph is a framework for building stateful applications with LLMs.\"),\n",
        "        HumanMessage(content=\"Can it handle multiple agents?\"),\n",
        "        AIMessage(content=\"Yes, LangGraph supports multi-agent workflows and coordination.\"),\n",
        "        HumanMessage(content=\"How does state management work?\"),\n",
        "        AIMessage(content=\"State is managed through TypedDict structures with proper annotations.\")\n",
        "    ],\n",
        "    \"summary\": \"Initial conversation about LangGraph\",\n",
        "    \"window_size\": 3\n",
        "}\n",
        "\n",
        "# Generate summary\n",
        "result = summary_generation(test_conversation)\n",
        "\n",
        "print(f\"Message count: {len(result['messages'])}\")\n",
        "print(f\"\\nGenerated Summary:\")\n",
        "print(f\"{result['summary']}\")\n",
        "print(f\"\\nSummary triggered at message count: {len(result['messages'])}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Key Takeaways\n",
        "\n",
        "### Summary Management\n",
        "- Use thresholds for efficient processing\n",
        "- Maintain context through summarization  \n",
        "- Integrate with message windowing\n",
        "\n",
        "### State Handling\n",
        "- Update summaries atomically\n",
        "- Preserve message context\n",
        "- Maintain state consistency\n",
        "\n",
        "### Common Pitfalls\n",
        "- Missing message validation\n",
        "- Inefficient summary generation\n",
        "- Poor summary format design\n",
        "- Incomplete state updates\n",
        "\n",
        "### Next Steps\n",
        "- Add intelligent summary compression\n",
        "- Implement topic extraction\n",
        "- Add summary-based routing\n",
        "- Integrate with LLM for better summaries\n",
        "\n",
        "# Variations and Extensions\n",
        "\n",
        "## Enhanced Summarization\n",
        "- **LLM-based semantic summarization**: Use language models for context-aware summaries\n",
        "- **Topic-based summary clustering**: Group related conversations by themes\n",
        "- **Example use case**: Complex conversation tracking across multiple sessions\n",
        "\n",
        "## Conditional Summarization  \n",
        "- **Topic-based summary triggers**: Generate summaries when conversation topics shift\n",
        "- **Importance-based summarization**: Prioritize key information in summaries\n",
        "- **Scenario**: Adaptive conversation management for dynamic contexts"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "vscode": {
          "languageId": "raw"
        }
      },
      "source": [
        "## Exercise 5: Building Advanced Multi-Node Message Processing Pipelines\n",
        "\n",
        "Now let's combine everything we've learned to build a multi-node processing pipeline. This demonstrates how to create modular, scalable systems by combining specialized nodes.\n",
        "\n",
        "### Pipeline Architecture:\n",
        "1. **Message Processor**: Initial message handling\n",
        "2. **Window Manager**: Memory optimization\n",
        "3. **Summarizer**: Context preservation\n",
        "4. **Response Generator**: Final output creation\n",
        "\n",
        "### Benefits of Multi-Node Architecture:\n",
        "- **Separation of Concerns**: Each node has a specific responsibility\n",
        "- **Testability**: Individual nodes can be tested in isolation\n",
        "- **Scalability**: Easy to add or modify nodes\n",
        "- **Maintainability**: Clear, modular code structure\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 32,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Initialize pipeline graph\n",
        "\n",
        "graph = StateGraph(State)\n",
        "\n",
        "# Add processing nodes\n",
        "graph.add_node(\"processor\", process_message)\n",
        "graph.add_node(\"windowing\", message_windowing)\n",
        "graph.add_node(\"summarizer\", summary_generation)\n",
        "# Configure pipeline flow\n",
        "graph.add_edge(START, \"processor\")\n",
        "graph.add_edge(\"processor\", \"windowing\")\n",
        "graph.add_edge(\"windowing\", \"summarizer\")\n",
        "graph.add_edge(\"summarizer\", END)\n",
        "# Compile for execution\n",
        "\n",
        "pipeline = graph.compile()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 33,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "Testing Multi-Node Pipeline:\n",
            "============================================================\n",
            "\n",
            "--- Turn 1 ---\n",
            "User: Hello! I'm interested in learning about LangGraph\n",
            "\n",
            "State Info:\n",
            "  Messages in memory: 1\n",
            "  Summary length: 0 chars\n",
            "\n",
            "--- Turn 2 ---\n",
            "User: What are the key features of LangGraph?\n",
            "\n",
            "State Info:\n",
            "  Messages in memory: 2\n",
            "  Summary length: 0 chars\n",
            "\n",
            "--- Turn 3 ---\n",
            "User: How does it compare to other frameworks?\n",
            "\n",
            "State Info:\n",
            "  Messages in memory: 3\n",
            "  Summary length: 158 chars\n",
            "\n",
            "--- Turn 4 ---\n",
            "User: Can you show me a simple example?\n",
            "\n",
            "State Info:\n",
            "  Messages in memory: 4\n",
            "  Summary length: 195 chars\n",
            "\n",
            "--- Turn 5 ---\n",
            "User: What about error handling?\n",
            "\n",
            "State Info:\n",
            "  Messages in memory: 5\n",
            "  Summary length: 225 chars\n"
          ]
        }
      ],
      "source": [
        "# Test the complete pipeline\n",
        "print(\"\\nTesting Multi-Node Pipeline:\")\n",
        "print(\"=\" * 60)\n",
        "\n",
        "# Initialize conversation state\n",
        "conversation_state = {\n",
        "    \"messages\": [],\n",
        "    \"summary\": \"\",\n",
        "    \"window_size\": 3\n",
        "}\n",
        "\n",
        "# Simulate a conversation\n",
        "test_messages = [\n",
        "    \"Hello! I'm interested in learning about LangGraph\",\n",
        "    \"What are the key features of LangGraph?\",\n",
        "    \"How does it compare to other frameworks?\",\n",
        "    \"Can you show me a simple example?\",\n",
        "    \"What about error handling?\"\n",
        "]\n",
        "\n",
        "for i, user_msg in enumerate(test_messages):\n",
        "    print(f\"\\n--- Turn {i+1} ---\")\n",
        "    print(f\"User: {user_msg}\")\n",
        "    \n",
        "    # Add user message to state\n",
        "    conversation_state[\"messages\"].append(HumanMessage(content=user_msg))\n",
        "    \n",
        "    # Process through pipeline\n",
        "    result = pipeline.invoke(conversation_state)\n",
        "    \n",
        "    # Update conversation state\n",
        "    conversation_state = result\n",
        "    \n",
        "    # Display the response\n",
        "    if result[\"messages\"]:\n",
        "        last_msg = result[\"messages\"][-1]\n",
        "        if isinstance(last_msg, AIMessage):\n",
        "            print(f\"Assistant: {last_msg.content[:150]}...\")\n",
        "    \n",
        "    print(f\"\\nState Info:\")\n",
        "    print(f\"  Messages in memory: {len(result['messages'])}\")\n",
        "    print(f\"  Summary length: {len(result['summary'])} chars\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "vscode": {
          "languageId": "raw"
        }
      },
      "source": [
        "## Putting It All Together: Production-Ready Patterns\n",
        "\n",
        "Let's combine all the concepts into a production-ready conversational agent that demonstrates best practices for state management and multi-node processing.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Window Manager\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 35,
      "metadata": {},
      "outputs": [],
      "source": [
        "def manage_message_window(state: State) -> State:\n",
        "    \"\"\"\n",
        "    Maintain optimal message history through window-based pruning.\n",
        "    \n",
        "    This function implements several key concepts:\n",
        "    1. Automatic message pruning\n",
        "    2. Window-based history management\n",
        "    3. State preservation during updates\n",
        "    \n",
        "    Args:\n",
        "        state: Current conversation state with messages and window configuration\n",
        "        \n",
        "    Returns:\n",
        "        State: Updated state with windowed message history\n",
        "    \"\"\"\n",
        "    current_window_size = state.get(\"window_size\", 5)\n",
        "    current_messages = state[\"messages\"]\n",
        "    \n",
        "    # Check if pruning is needed\n",
        "    if len(current_messages) > current_window_size:\n",
        "        # Before pruning, update summary with older messages\n",
        "        older_messages = current_messages[:-current_window_size]\n",
        "        summary_addition = \" Previous context: \" + \"; \".join([\n",
        "            f\"{type(msg).__name__}: {msg.content[:50]}...\" \n",
        "            for msg in older_messages\n",
        "        ])\n",
        "        \n",
        "        # Prune messages to window size\n",
        "        pruned_messages = current_messages[-current_window_size:]\n",
        "        \n",
        "        # Update state with pruned messages and enhanced summary\n",
        "        return {\n",
        "            \"messages\": pruned_messages,\n",
        "            \"summary\": state[\"summary\"] + summary_addition,\n",
        "            \"window_size\": current_window_size\n",
        "        }\n",
        "    \n",
        "    # No pruning needed\n",
        "    return state\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Dynamic summary\n",
        "```bash\n",
        "\n",
        "```"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 34,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Initialize LLM for summarization\n",
        "llm = ChatOpenAI(model=\"gpt-4o-mini\", temperature=0)\n",
        "\n",
        "def generate_dynamic_summary(state: State) -> State:\n",
        "    \"\"\"\n",
        "    Generate and maintain dynamic conversation summaries.\n",
        "    \n",
        "    This function implements:\n",
        "    1. Threshold-based summary generation\n",
        "    2. Context preservation through summarization\n",
        "    3. Integration with LLM for intelligent summaries\n",
        "    \n",
        "    Args:\n",
        "        state: Current conversation state\n",
        "        \n",
        "    Returns:\n",
        "        State: Updated state with new summary\n",
        "    \"\"\"\n",
        "    messages = state[\"messages\"]\n",
        "    \n",
        "    # Only summarize if we have enough messages\n",
        "    if len(messages) < 3:\n",
        "        return state\n",
        "    \n",
        "    # Check if summary needs updating (every N messages)\n",
        "    if len(messages) % 3 == 0:\n",
        "        # Create conversation text for summarization\n",
        "        conversation_text = \"\\n\".join([\n",
        "            f\"{type(msg).__name__.replace('Message', '')}: {msg.content}\"\n",
        "            for msg in messages[-5:]  # Summarize last 5 messages\n",
        "        ])\n",
        "        \n",
        "        # Use LLM to generate summary\n",
        "        summary_prompt = PromptTemplate(\n",
        "            input_variables=[\"conversation\", \"previous_summary\"],\n",
        "            template=\"\"\"Based on the previous summary and recent conversation, create a concise updated summary.\n",
        "\n",
        "Previous Summary: {previous_summary}\n",
        "\n",
        "Recent Conversation:\n",
        "{conversation}\n",
        "\n",
        "Updated Summary (2-3 sentences):\"\"\"\n",
        "        )\n",
        "        \n",
        "        summary_response = llm.invoke(\n",
        "            summary_prompt.format(\n",
        "                conversation=conversation_text,\n",
        "                previous_summary=state.get(\"summary\", \"No previous summary\")\n",
        "            )\n",
        "        )\n",
        "        \n",
        "        return {\n",
        "            \"messages\": messages,\n",
        "            \"summary\": summary_response.content,\n",
        "            \"window_size\": state[\"window_size\"]\n",
        "        }\n",
        "    \n",
        "    return state\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Define nodes \n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {},
      "outputs": [],
      "source": [
        "class ProductionState(TypedDict):\n",
        "    \"\"\"\n",
        "    Production-ready state with comprehensive tracking.\n",
        "    \"\"\"\n",
        "    messages: Annotated[list[BaseMessage], add_messages]\n",
        "    summary: str\n",
        "    window_size: int\n",
        "    conversation_metadata: dict  # Track conversation metrics\n",
        "    user_context: dict  # Store user preferences/info\n",
        "\n",
        "def create_production_agent():\n",
        "    \"\"\"\n",
        "    Create a production-ready conversational agent with all best practices.\n",
        "    \"\"\"\n",
        "    graph = StateGraph(ProductionState)\n",
        "    \n",
        "    # Enhanced message processor with metadata tracking\n",
        "    def enhanced_processor(state: ProductionState) -> ProductionState:\n",
        "        \"\"\"Process messages and update metadata.\"\"\"\n",
        "        metadata = state.get(\"conversation_metadata\", {})\n",
        "        metadata[\"total_messages\"] = metadata.get(\"total_messages\", 0) + 1\n",
        "        metadata[\"last_updated\"] = \"now\"\n",
        "        \n",
        "        # Extract user context from messages\n",
        "        user_context = state.get(\"user_context\", {})\n",
        "        if state[\"messages\"]:\n",
        "            last_msg = state[\"messages\"][-1]\n",
        "            if isinstance(last_msg, HumanMessage):\n",
        "                # Simple intent detection\n",
        "                content_lower = last_msg.content.lower()\n",
        "                if \"my name is\" in content_lower:\n",
        "                    # Extract name (simplified)\n",
        "                    words = last_msg.content.split()\n",
        "                    if \"is\" in words:\n",
        "                        idx = words.index(\"is\")\n",
        "                        if idx < len(words) - 1:\n",
        "                            user_context[\"name\"] = words[idx + 1].strip(\".,!?\")\n",
        "        \n",
        "        return {\n",
        "            **state,\n",
        "            \"conversation_metadata\": metadata,\n",
        "            \"user_context\": user_context\n",
        "        }\n",
        "    \n",
        "    # Smart response generator using all context\n",
        "    def smart_response_generator(state: ProductionState) -> ProductionState:\n",
        "        \"\"\"Generate responses using full context.\"\"\"\n",
        "        last_msg = state[\"messages\"][-1] if state[\"messages\"] else None\n",
        "        \n",
        "        if isinstance(last_msg, HumanMessage):\n",
        "            # Build comprehensive context\n",
        "            context_parts = [\n",
        "                f\"Summary: {state.get('summary', 'No summary yet')}\"\n",
        "            ]\n",
        "            \n",
        "            # Add user context if available\n",
        "            if state.get(\"user_context\", {}).get(\"name\"):\n",
        "                context_parts.append(f\"User name: {state['user_context']['name']}\")\n",
        "            \n",
        "            # Add conversation metadata\n",
        "            metadata = state.get(\"conversation_metadata\", {})\n",
        "            if metadata.get(\"total_messages\"):\n",
        "                context_parts.append(f\"Messages exchanged: {metadata['total_messages']}\")\n",
        "            \n",
        "            context_parts.append(f\"User message: {last_msg.content}\")\n",
        "            \n",
        "            full_context = \"\\n\".join(context_parts)\n",
        "            \n",
        "            prompt = PromptTemplate(\n",
        "                input_variables=[\"context\"],\n",
        "                template=\"\"\"You are a helpful assistant. Use all available context to provide personalized responses.\n",
        "                `\n",
        "                Context:\n",
        "                {context}\n",
        "\n",
        "                Provide a helpful, personalized response:\"\"\"`\n",
        "            )\n",
        "            \n",
        "            response = llm.invoke(prompt.format(context=full_context))\n",
        "            \n",
        "            return {\n",
        "                **state,\n",
        "                \"messages\": [AIMessage(content=response.content)]\n",
        "            }\n",
        "        \n",
        "        return state\n",
        "    \n",
        "    # Add nodes\n",
        "    graph.add_node(\"enhanced_processor\", enhanced_processor)\n",
        "    graph.add_node(\"window_manager\", window_manager_node)\n",
        "    graph.add_node(\"summarizer\", summarizer_node)\n",
        "    graph.add_node(\"smart_responder\", smart_response_generator)\n",
        "    \n",
        "    # Define flow\n",
        "    graph.add_edge(START, \"enhanced_processor\")\n",
        "    graph.add_edge(\"enhanced_processor\", \"window_manager\")\n",
        "    graph.add_edge(\"window_manager\", \"summarizer\")\n",
        "    graph.add_edge(\"summarizer\", \"smart_responder\")\n",
        "    graph.add_edge(\"smart_responder\", END)\n",
        "    \n",
        "    return graph.compile()\n",
        "\n",
        "# Create production agent\n",
        "production_agent = create_production_agent()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "Testing Production Agent:\n",
            "============================================================\n",
            "\n",
            "--- Turn 1 ---\n",
            "User: Hello! My name is Alex\n",
            "[Window Manager] Current messages: 1, Window: 4\n",
            "Assistant: Hello, Alex! It's great to meet you! How can I assist you today? If there's anything specific you'd like to talk about or any questions you have, feel free to share!...\n",
            "\n",
            "Enhanced State:\n",
            "  Window: 2/4\n",
            "  User Context: {'name': 'Alex'}\n",
            "  Metadata: {'total_messages': 1, 'last_updated': 'now'}\n",
            "\n",
            "--- Turn 2 ---\n",
            "User: I want to learn about building agents with LangGraph\n",
            "[Window Manager] Current messages: 3, Window: 4\n",
            "[Summarizer] Generating summary for 3 messages\n",
            "Assistant: Hi Alex! I'm excited to help you learn about building agents with LangGraph. To get started, it would be helpful to know what specific aspects you're interested in. Are you looking for an overview of ...\n",
            "\n",
            "Enhanced State:\n",
            "  Window: 4/4\n",
            "  User Context: {'name': 'Alex'}\n",
            "  Metadata: {'total_messages': 2, 'last_updated': 'now'}\n",
            "\n",
            "--- Turn 3 ---\n",
            "User: What makes it different from regular chatbots?\n",
            "[Window Manager] Current messages: 5, Window: 4\n",
            "[Summarizer] Generating summary for 5 messages\n",
            "Assistant: Hi Alex! Great question! LangGraph differs from regular chatbots in several key ways. \n",
            "\n",
            "1. **Contextual Understanding**: LangGraph is designed to maintain a deeper understanding of context over longer...\n",
            "\n",
            "Enhanced State:\n",
            "  Window: 6/4\n",
            "  User Context: {'name': 'Alex'}\n",
            "  Metadata: {'total_messages': 3, 'last_updated': 'now'}\n",
            "\n",
            "--- Turn 4 ---\n",
            "User: Can you give me a specific example?\n",
            "[Window Manager] Current messages: 7, Window: 4\n",
            "[Summarizer] Generating summary for 7 messages\n",
            "Assistant: Absolutely, Alex! Lets dive into a specific example of building an agent using LangGraph.\n",
            "\n",
            "Imagine you want to create a simple chatbot agent that can assist users in booking appointments. Heres a st...\n",
            "\n",
            "Enhanced State:\n",
            "  Window: 8/4\n",
            "  User Context: {'name': 'Alex'}\n",
            "  Metadata: {'total_messages': 4, 'last_updated': 'now'}\n",
            "\n",
            "--- Turn 5 ---\n",
            "User: Thanks, this is really helpful!\n",
            "[Window Manager] Current messages: 9, Window: 4\n",
            "[Summarizer] Generating summary for 9 messages\n",
            "Assistant: You're very welcome, Alex! I'm glad to hear that you found the information helpful. If you have any more questions about building agents with LangGraph or if there's a specific aspect you'd like to di...\n",
            "\n",
            "Enhanced State:\n",
            "  Window: 10/4\n",
            "  User Context: {'name': 'Alex'}\n",
            "  Metadata: {'total_messages': 5, 'last_updated': 'now'}\n"
          ]
        }
      ],
      "source": [
        "# Test the production agent\n",
        "print(\"\\nTesting Production Agent:\")\n",
        "print(\"=\" * 60)\n",
        "\n",
        "# Initialize state\n",
        "prod_state = {\n",
        "    \"messages\": [],\n",
        "    \"summary\": \"\",\n",
        "    \"window_size\": 4,\n",
        "    \"conversation_metadata\": {},\n",
        "    \"user_context\": {}\n",
        "}\n",
        "\n",
        "# Simulate a personalized conversation\n",
        "production_messages = [\n",
        "    \"Hello! My name is Alex\",\n",
        "    \"I want to learn about building agents with LangGraph\",\n",
        "    \"What makes it different from regular chatbots?\",\n",
        "    \"Can you give me a specific example?\",\n",
        "    \"Thanks, this is really helpful!\"\n",
        "]\n",
        "\n",
        "for i, msg in enumerate(production_messages):\n",
        "    print(f\"\\n--- Turn {i+1} ---\")\n",
        "    print(f\"User: {msg}\")\n",
        "    \n",
        "    # Add message and process\n",
        "    prod_state[\"messages\"].append(HumanMessage(content=msg))\n",
        "    result = production_agent.invoke(prod_state)\n",
        "    prod_state = result\n",
        "    \n",
        "    # Show response\n",
        "    if result[\"messages\"]:\n",
        "        last_msg = result[\"messages\"][-1]\n",
        "        if isinstance(last_msg, AIMessage):\n",
        "            print(f\"Assistant: {last_msg.content[:200]}...\")\n",
        "    \n",
        "    # Show enhanced state info\n",
        "    print(f\"\\nEnhanced State:\")\n",
        "    print(f\"  Window: {len(result['messages'])}/{result['window_size']}\")\n",
        "    print(f\"  User Context: {result.get('user_context', {})}\")\n",
        "    print(f\"  Metadata: {result.get('conversation_metadata', {})}\")\n"
      ]
    },
    {
      "cell_type": "raw",
      "metadata": {
        "vscode": {
          "languageId": "raw"
        }
      },
      "source": [
        "## Key Takeaways and Best Practices\n",
        "\n",
        "### 1. **Advanced State Management**\n",
        "- Use TypedDict for type-safe state definitions\n",
        "- Include fields beyond messages (summaries, metadata, user context)\n",
        "- Always preserve all state fields during updates\n",
        "- Initialize state fields with sensible defaults\n",
        "\n",
        "### 2. **Memory Optimization**\n",
        "- Implement sliding windows to control memory usage\n",
        "- Generate summaries before pruning messages\n",
        "- Balance window size with context needs\n",
        "- Consider different window sizes for different use cases\n",
        "\n",
        "### 3. **Multi-Node Architecture**\n",
        "- Separate concerns into specialized nodes\n",
        "- Make nodes composable and reusable\n",
        "- Use clear naming conventions\n",
        "- Document node responsibilities\n",
        "\n",
        "### 4. **Production Patterns**\n",
        "- Track conversation metadata for analytics\n",
        "- Store user context for personalization\n",
        "- Implement comprehensive error handling\n",
        "- Use proper logging for debugging\n",
        "\n",
        "### 5. **Common Pitfalls to Avoid**\n",
        "-  Forgetting to preserve state fields\n",
        "-  Not handling empty states\n",
        "-  Ignoring memory constraints\n",
        "-  Creating monolithic nodes\n",
        "-  Missing error handling\n",
        "\n",
        "### 6. **Performance Considerations**\n",
        "- Optimize summary generation frequency\n",
        "- Cache frequently accessed data\n",
        "- Monitor state size growth\n",
        "- Profile node execution times\n"
      ]
    },
    {
      "cell_type": "raw",
      "metadata": {
        "vscode": {
          "languageId": "raw"
        }
      },
      "source": [
        "## Next Steps\n",
        "\n",
        "Now that you've mastered advanced state management and multi-node processing:\n",
        "\n",
        "### 1. **Experiment with Different Architectures**\n",
        "- Try parallel node execution\n",
        "- Implement conditional routing\n",
        "- Create branching pipelines\n",
        "\n",
        "### 2. **Add Advanced Features**\n",
        "- Implement state persistence\n",
        "- Add conversation analytics\n",
        "- Create custom state validators\n",
        "\n",
        "### 3. **Optimize for Production**\n",
        "- Add comprehensive error handling\n",
        "- Implement retry logic\n",
        "- Create health checks\n",
        "\n",
        "### 4. **Explore Integration**\n",
        "- Connect to external APIs\n",
        "- Add tool usage capabilities\n",
        "- Implement multi-agent coordination\n",
        "\n",
        "### 5. **Study Real-World Examples**\n",
        "- Review the LangGraph documentation\n",
        "- Explore community examples\n",
        "- Build your own production system\n",
        "\n",
        "## Resources\n",
        "\n",
        "- [LangGraph Documentation](https://langchain-ai.github.io/langgraph/)\n",
        "- [AI Product Engineer Tutorials](https://aiproduct.engineer/tutorials/)\n",
        "- [LangChain Community](https://github.com/langchain-ai/langchain)\n",
        "\n",
        "## Conclusion\n",
        "\n",
        "Unit 12 has equipped you with advanced patterns for building sophisticated conversational agents. You've learned how to:\n",
        "- Design complex state structures\n",
        "- Implement memory-efficient processing\n",
        "- Build modular, scalable pipelines\n",
        "- Apply production-ready patterns\n",
        "\n",
        "These skills form the foundation for building real-world AI applications that can handle complex conversations, maintain context efficiently, and scale to production workloads.\n",
        "\n",
        "Happy building! \n"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": ".venv",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.13.3"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 2
}
