{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "vscode": {
          "languageId": "raw"
        }
      },
      "source": [
        "\n",
        "# LangGraph 2.2 Tutorial: Advanced State Management and Multi-Node Processing in Production\n",
        "\n",
        "Let's combine all the concepts into a production-ready conversational agent that demonstrates best practices for state management and multi-node processing.\n",
        "\n",
        "### Pipeline Architecture:\n",
        "1. **Message Processor**: Initial message handling\n",
        "2. **Window Manager**: Memory optimization\n",
        "3. **Summarizer**: Context preservation\n",
        "4. **Response Generator**: Final output creation\n",
        "\n",
        "### Benefits of Multi-Node Architecture:\n",
        "- **Separation of Concerns**: Each node has a specific responsibility\n",
        "- **Testability**: Individual nodes can be tested in isolation\n",
        "- **Scalability**: Easy to add or modify nodes\n",
        "- **Maintainability**: Clear, modular code structure"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "vscode": {
          "languageId": "raw"
        }
      },
      "source": [
        "## Setting Up Our Environment\n",
        "\n",
        "Before we dive into advanced patterns, let's ensure our environment is properly configured with all necessary dependencies.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 23,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Note: you may need to restart the kernel to use updated packages.\n"
          ]
        }
      ],
      "source": [
        "# Install required packages\n",
        "%pip install langgraph langchain langchain-openai python-dotenv --quiet\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 24,
      "metadata": {},
      "outputs": [],
      "source": [
        "import os\n",
        "from typing import Annotated, TypedDict, List\n",
        "from dotenv import load_dotenv\n",
        "\n",
        "# Load environment variables\n",
        "load_dotenv()\n",
        "\n",
        "# Set API keys\n",
        "os.environ[\"OPENAI_API_KEY\"] = os.getenv('OPENAI_API_KEY')\n",
        "\n",
        "# Import core dependencies\n",
        "from langchain_core.messages import BaseMessage, HumanMessage, AIMessage, SystemMessage\n",
        "from langgraph.graph import StateGraph, START, END\n",
        "from langgraph.graph.message import add_messages\n",
        "from langchain_openai import ChatOpenAI\n",
        "from langchain.prompts import PromptTemplate\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 25,
      "metadata": {},
      "outputs": [],
      "source": [
        "class State(TypedDict):\n",
        "    \"\"\"\n",
        "    Enhanced state container with context management capabilities.\n",
        "    \n",
        "    This implementation demonstrates advanced state management by tracking:\n",
        "    1. Message history with proper LangGraph annotations\n",
        "    2. Conversation summaries for context retention\n",
        "    3. Memory window control for efficient processing\n",
        "    \n",
        "    Attributes:\n",
        "        messages: List of conversation messages with LangGraph's add_messages\n",
        "                 annotation for proper message handling\n",
        "        summary: Running summary of the conversation context\n",
        "        window_size: Control parameter for message history retention\n",
        "        \n",
        "    Note:\n",
        "        The add_messages annotation is crucial for proper message\n",
        "        handling in LangGraph, ensuring correct state updates while\n",
        "        the additional fields enable sophisticated conversation management.\n",
        "    \"\"\"\n",
        "    messages: Annotated[list[BaseMessage], add_messages]\n",
        "    summary: str\n",
        "    window_size: int\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 26,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Initialize LLM for summarization\n",
        "llm = ChatOpenAI(model=\"gpt-4o-mini\", temperature=0)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Window Manager\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 27,
      "metadata": {},
      "outputs": [],
      "source": [
        "def manage_message_window(state: State) -> State:\n",
        "    \"\"\"\n",
        "    Maintain optimal message history through window-based pruning.\n",
        "    \n",
        "    This function implements several key concepts:\n",
        "    1. Automatic message pruning\n",
        "    2. Window-based history management\n",
        "    3. State preservation during updates\n",
        "    \n",
        "    Args:\n",
        "        state: Current conversation state with messages and window configuration\n",
        "        \n",
        "    Returns:\n",
        "        State: Updated state with windowed message history\n",
        "    \"\"\"\n",
        "    current_window_size = state.get(\"window_size\", 5)\n",
        "    current_messages = state[\"messages\"]\n",
        "    \n",
        "    # Check if pruning is needed\n",
        "    if len(current_messages) > current_window_size:\n",
        "        # Before pruning, update summary with older messages\n",
        "        older_messages = current_messages[:-current_window_size]\n",
        "        summary_addition = \" Previous context: \" + \"; \".join([\n",
        "            f\"{type(msg).__name__}: {msg.content[:50]}...\" \n",
        "            for msg in older_messages\n",
        "        ])\n",
        "        \n",
        "        # Prune messages to window size\n",
        "        pruned_messages = current_messages[-current_window_size:]\n",
        "        \n",
        "        # Update state with pruned messages and enhanced summary\n",
        "        return {\n",
        "            \"messages\": pruned_messages,\n",
        "            \"summary\": state[\"summary\"] + summary_addition,\n",
        "            \"window_size\": current_window_size\n",
        "        }\n",
        "    \n",
        "    # No pruning needed\n",
        "    return state\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Dynamic summary\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 28,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Initialize LLM for summarization\n",
        "llm = ChatOpenAI(model=\"gpt-4o-mini\", temperature=0)\n",
        "\n",
        "def generate_dynamic_summary(state: State) -> State:\n",
        "    \"\"\"\n",
        "    Generate and maintain dynamic conversation summaries.\n",
        "    \n",
        "    This function implements:\n",
        "    1. Threshold-based summary generation\n",
        "    2. Context preservation through summarization\n",
        "    3. Integration with LLM for intelligent summaries\n",
        "    \n",
        "    Args:\n",
        "        state: Current conversation state\n",
        "        \n",
        "    Returns:\n",
        "        State: Updated state with new summary\n",
        "    \"\"\"\n",
        "    messages = state[\"messages\"]\n",
        "    \n",
        "    # Only summarize if we have enough messages\n",
        "    if len(messages) < 3:\n",
        "        return state\n",
        "    \n",
        "    # Check if summary needs updating (every N messages)\n",
        "    if len(messages) % 3 == 0:\n",
        "        # Create conversation text for summarization\n",
        "        conversation_text = \"\\n\".join([\n",
        "            f\"{type(msg).__name__.replace('Message', '')}: {msg.content}\"\n",
        "            for msg in messages[-5:]  # Summarize last 5 messages\n",
        "        ])\n",
        "        \n",
        "        # Use LLM to generate summary\n",
        "        summary_prompt = PromptTemplate(\n",
        "            input_variables=[\"conversation\", \"previous_summary\"],\n",
        "            template=\"\"\"Based on the previous summary and recent conversation, create a concise updated summary.\n",
        "\n",
        "Previous Summary: {previous_summary}\n",
        "\n",
        "Recent Conversation:\n",
        "{conversation}\n",
        "\n",
        "Updated Summary (2-3 sentences):\"\"\"\n",
        "        )\n",
        "        \n",
        "        summary_response = llm.invoke(\n",
        "            summary_prompt.format(\n",
        "                conversation=conversation_text,\n",
        "                previous_summary=state.get(\"summary\", \"No previous summary\")\n",
        "            )\n",
        "        )\n",
        "        \n",
        "        return {\n",
        "            \"messages\": messages,\n",
        "            \"summary\": summary_response.content,\n",
        "            \"window_size\": state[\"window_size\"]\n",
        "        }\n",
        "    \n",
        "    return state\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Define nodes \n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 29,
      "metadata": {},
      "outputs": [],
      "source": [
        "\n",
        "# Define specialized nodes for our pipeline\n",
        "\n",
        "def message_processor_node(state: State) -> State:\n",
        "    \"\"\"\n",
        "    Initial message processing node.\n",
        "    Handles new messages and prepares them for the pipeline.\n",
        "    \"\"\"\n",
        "    if not state[\"messages\"]:\n",
        "        return {\n",
        "            \"messages\": [SystemMessage(content=\"System initialized\")],\n",
        "            \"summary\": \"New conversation started\",\n",
        "            \"window_size\": state.get(\"window_size\", 5)\n",
        "        }\n",
        "    \n",
        "    # Process the last human message if present\n",
        "    last_human_msg = None\n",
        "    for msg in reversed(state[\"messages\"]):\n",
        "        if isinstance(msg, HumanMessage):\n",
        "            last_human_msg = msg\n",
        "            break\n",
        "    \n",
        "    if last_human_msg:\n",
        "        # Add processing metadata\n",
        "        print(f\"[Processor] Processing: {last_human_msg.content[:50]}...\")\n",
        "    \n",
        "    return state\n",
        "\n",
        "def window_manager_node(state: State) -> State:\n",
        "    \"\"\"\n",
        "    Window management node.\n",
        "    Ensures message history stays within configured limits.\n",
        "    \"\"\"\n",
        "    print(f\"[Window Manager] Current messages: {len(state['messages'])}, Window: {state['window_size']}\")\n",
        "    return manage_message_window(state)\n",
        "\n",
        "def summarizer_node(state: State) -> State:\n",
        "    \"\"\"\n",
        "    Summarization node.\n",
        "    Creates or updates conversation summaries.\n",
        "    \"\"\"\n",
        "    # Only summarize if we have enough messages\n",
        "    if len(state[\"messages\"]) >= 3:\n",
        "        print(f\"[Summarizer] Generating summary for {len(state['messages'])} messages\")\n",
        "        return generate_dynamic_summary(state)\n",
        "    return state\n",
        "\n",
        "def response_generator_node(state: State) -> State:\n",
        "    \"\"\"\n",
        "    Response generation node.\n",
        "    Creates appropriate responses based on the processed state.\n",
        "    \"\"\"\n",
        "    # Check if we need to generate a response\n",
        "    last_msg = state[\"messages\"][-1] if state[\"messages\"] else None\n",
        "    \n",
        "    if isinstance(last_msg, HumanMessage):\n",
        "        # Generate context-aware response\n",
        "        context = f\"Summary: {state['summary']}\\nUser: {last_msg.content}\"\n",
        "        \n",
        "        prompt = PromptTemplate(\n",
        "            input_variables=[\"context\"],\n",
        "            template=\"\"\"Based on the conversation context, provide a helpful response.\n",
        "\n",
        "{context}\n",
        "\n",
        "Response:\"\"\"\n",
        "        )\n",
        "        \n",
        "        response = llm.invoke(prompt.format(context=context))\n",
        "        \n",
        "        print(f\"[Response Generator] Generated response\")\n",
        "        \n",
        "        return {\n",
        "            \"messages\": [AIMessage(content=response.content)],\n",
        "            \"summary\": state[\"summary\"],\n",
        "            \"window_size\": state[\"window_size\"]\n",
        "        }\n",
        "    \n",
        "    return state\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 30,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Define specialized nodes for our pipeline\n",
        "\n",
        "def message_processor_node(state: State) -> State:\n",
        "    \"\"\"\n",
        "    Initial message processing node.\n",
        "    Handles new messages and prepares them for the pipeline.\n",
        "    \"\"\"\n",
        "    if not state[\"messages\"]:\n",
        "        return {\n",
        "            \"messages\": [SystemMessage(content=\"System initialized\")],\n",
        "            \"summary\": \"New conversation started\",\n",
        "            \"window_size\": state.get(\"window_size\", 5)\n",
        "        }\n",
        "    \n",
        "    # Process the last human message if present\n",
        "    last_human_msg = None\n",
        "    for msg in reversed(state[\"messages\"]):\n",
        "        if isinstance(msg, HumanMessage):\n",
        "            last_human_msg = msg\n",
        "            break\n",
        "    \n",
        "    if last_human_msg:\n",
        "        # Add processing metadata\n",
        "        print(f\"[Processor] Processing: {last_human_msg.content[:50]}...\")\n",
        "    \n",
        "    return state\n",
        "\n",
        "def window_manager_node(state: State) -> State:\n",
        "    \"\"\"\n",
        "    Window management node.\n",
        "    Ensures message history stays within configured limits.\n",
        "    \"\"\"\n",
        "    print(f\"[Window Manager] Current messages: {len(state['messages'])}, Window: {state['window_size']}\")\n",
        "    return manage_message_window(state)\n",
        "\n",
        "def summarizer_node(state: State) -> State:\n",
        "    \"\"\"\n",
        "    Summarization node.\n",
        "    Creates or updates conversation summaries.\n",
        "    \"\"\"\n",
        "    # Only summarize if we have enough messages\n",
        "    if len(state[\"messages\"]) >= 3:\n",
        "        print(f\"[Summarizer] Generating summary for {len(state['messages'])} messages\")\n",
        "        return generate_dynamic_summary(state)\n",
        "    return state\n",
        "\n",
        "def response_generator_node(state: State) -> State:\n",
        "    \"\"\"\n",
        "    Response generation node.\n",
        "    Creates appropriate responses based on the processed state.\n",
        "    \"\"\"\n",
        "    # Check if we need to generate a response\n",
        "    last_msg = state[\"messages\"][-1] if state[\"messages\"] else None\n",
        "    \n",
        "    if isinstance(last_msg, HumanMessage):\n",
        "        # Generate context-aware response\n",
        "        context = f\"Summary: {state['summary']}\\nUser: {last_msg.content}\"\n",
        "        \n",
        "        prompt = PromptTemplate(\n",
        "            input_variables=[\"context\"],\n",
        "            template=\"\"\"Based on the conversation context, provide a helpful response.\n",
        "\n",
        "{context}\n",
        "\n",
        "Response:\"\"\"\n",
        "        )\n",
        "        \n",
        "        response = llm.invoke(prompt.format(context=context))\n",
        "        \n",
        "        print(f\"[Response Generator] Generated response\")\n",
        "        \n",
        "        return {\n",
        "            \"messages\": [AIMessage(content=response.content)],\n",
        "            \"summary\": state[\"summary\"],\n",
        "            \"window_size\": state[\"window_size\"]\n",
        "        }\n",
        "    \n",
        "    return state\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 31,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "image/png": "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",
            "text/plain": [
              "<IPython.core.display.Image object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "# Build the multi-node pipeline\n",
        "def create_advanced_pipeline():\n",
        "    \"\"\"\n",
        "    Create a complete multi-node processing pipeline.\n",
        "    \"\"\"\n",
        "    # Initialize the graph\n",
        "    graph = StateGraph(State)\n",
        "    \n",
        "    # Add all nodes\n",
        "    graph.add_node(\"processor\", message_processor_node)\n",
        "    graph.add_node(\"window_manager\", window_manager_node)\n",
        "    graph.add_node(\"summarizer\", summarizer_node)\n",
        "    graph.add_node(\"response_generator\", response_generator_node)\n",
        "    \n",
        "    # Define the flow\n",
        "    graph.add_edge(START, \"processor\")\n",
        "    graph.add_edge(\"processor\", \"window_manager\")\n",
        "    graph.add_edge(\"window_manager\", \"summarizer\")\n",
        "    graph.add_edge(\"summarizer\", \"response_generator\")\n",
        "    graph.add_edge(\"response_generator\", END)\n",
        "    \n",
        "    # Compile the graph\n",
        "    return graph.compile()\n",
        "\n",
        "# Create and visualize the pipeline\n",
        "pipeline = create_advanced_pipeline()\n",
        "\n",
        "# Visualize the pipeline structure\n",
        "from IPython.display import display, Image\n",
        "from langchain_core.runnables.graph import MermaidDrawMethod\n",
        "\n",
        "try:\n",
        "    display(\n",
        "        Image(\n",
        "            pipeline.get_graph().draw_mermaid_png(\n",
        "                draw_method=MermaidDrawMethod.API,\n",
        "            )\n",
        "        )\n",
        "    )\n",
        "except Exception as e:\n",
        "    print(f\"Visualization not available: {e}\")\n",
        "    print(\"\\nPipeline structure:\")\n",
        "    print(\"START -> processor -> window_manager -> summarizer -> response_generator -> END\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 32,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "Testing Multi-Node Pipeline:\n",
            "============================================================\n",
            "\n",
            "--- Turn 1 ---\n",
            "User: Hello! I'm interested in learning about LangGraph\n",
            "[Processor] Processing: Hello! I'm interested in learning about LangGraph...\n",
            "[Window Manager] Current messages: 1, Window: 3\n",
            "[Response Generator] Generated response\n",
            "Assistant: Hello! LangGraph is a framework designed to facilitate the development of applications that leverage language models and graph-based data structures. ...\n",
            "\n",
            "State Info:\n",
            "  Messages in memory: 2\n",
            "  Summary length: 0 chars\n",
            "\n",
            "--- Turn 2 ---\n",
            "User: What are the key features of LangGraph?\n",
            "[Processor] Processing: What are the key features of LangGraph?...\n",
            "[Window Manager] Current messages: 3, Window: 3\n",
            "[Summarizer] Generating summary for 3 messages\n",
            "[Response Generator] Generated response\n",
            "Assistant: LangGraph offers several key features that enhance the development of applications using language models and graph-based data structures:\n",
            "\n",
            "1. **Graph ...\n",
            "\n",
            "State Info:\n",
            "  Messages in memory: 4\n",
            "  Summary length: 409 chars\n",
            "\n",
            "--- Turn 3 ---\n",
            "User: How does it compare to other frameworks?\n",
            "[Processor] Processing: How does it compare to other frameworks?...\n",
            "[Window Manager] Current messages: 5, Window: 3\n",
            "[Summarizer] Generating summary for 5 messages\n"
          ]
        },
        {
          "ename": "KeyboardInterrupt",
          "evalue": "",
          "output_type": "error",
          "traceback": [
            "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
            "\u001b[31mKeyboardInterrupt\u001b[39m                         Traceback (most recent call last)",
            "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[32]\u001b[39m\u001b[32m, line 29\u001b[39m\n\u001b[32m     26\u001b[39m conversation_state[\u001b[33m\"\u001b[39m\u001b[33mmessages\u001b[39m\u001b[33m\"\u001b[39m].append(HumanMessage(content=user_msg))\n\u001b[32m     28\u001b[39m \u001b[38;5;66;03m# Process through pipeline\u001b[39;00m\n\u001b[32m---> \u001b[39m\u001b[32m29\u001b[39m result = \u001b[43mpipeline\u001b[49m\u001b[43m.\u001b[49m\u001b[43minvoke\u001b[49m\u001b[43m(\u001b[49m\u001b[43mconversation_state\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     31\u001b[39m \u001b[38;5;66;03m# Update conversation state\u001b[39;00m\n\u001b[32m     32\u001b[39m conversation_state = result\n",
            "\u001b[36mFile \u001b[39m\u001b[32m~/agents-0-to-1/.venv/lib/python3.13/site-packages/langgraph/pregel/__init__.py:2719\u001b[39m, in \u001b[36mPregel.invoke\u001b[39m\u001b[34m(self, input, config, stream_mode, output_keys, interrupt_before, interrupt_after, checkpoint_during, debug, **kwargs)\u001b[39m\n\u001b[32m   2716\u001b[39m chunks: \u001b[38;5;28mlist\u001b[39m[Union[\u001b[38;5;28mdict\u001b[39m[\u001b[38;5;28mstr\u001b[39m, Any], Any]] = []\n\u001b[32m   2717\u001b[39m interrupts: \u001b[38;5;28mlist\u001b[39m[Interrupt] = []\n\u001b[32m-> \u001b[39m\u001b[32m2719\u001b[39m \u001b[43m\u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mchunk\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mstream\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m   2720\u001b[39m \u001b[43m    \u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[32m   2721\u001b[39m \u001b[43m    \u001b[49m\u001b[43mconfig\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   2722\u001b[39m \u001b[43m    \u001b[49m\u001b[43mstream_mode\u001b[49m\u001b[43m=\u001b[49m\u001b[43mstream_mode\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   2723\u001b[39m \u001b[43m    \u001b[49m\u001b[43moutput_keys\u001b[49m\u001b[43m=\u001b[49m\u001b[43moutput_keys\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   2724\u001b[39m \u001b[43m    \u001b[49m\u001b[43minterrupt_before\u001b[49m\u001b[43m=\u001b[49m\u001b[43minterrupt_before\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   2725\u001b[39m \u001b[43m    \u001b[49m\u001b[43minterrupt_after\u001b[49m\u001b[43m=\u001b[49m\u001b[43minterrupt_after\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   2726\u001b[39m \u001b[43m    \u001b[49m\u001b[43mcheckpoint_during\u001b[49m\u001b[43m=\u001b[49m\u001b[43mcheckpoint_during\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   2727\u001b[39m \u001b[43m    \u001b[49m\u001b[43mdebug\u001b[49m\u001b[43m=\u001b[49m\u001b[43mdebug\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   2728\u001b[39m \u001b[43m    \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   2729\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\u001b[43m:\u001b[49m\n\u001b[32m   2730\u001b[39m \u001b[43m    \u001b[49m\u001b[38;5;28;43;01mif\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mstream_mode\u001b[49m\u001b[43m \u001b[49m\u001b[43m==\u001b[49m\u001b[43m \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mvalues\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\n\u001b[32m   2731\u001b[39m \u001b[43m        \u001b[49m\u001b[38;5;28;43;01mif\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m   2732\u001b[39m \u001b[43m            \u001b[49m\u001b[38;5;28;43misinstance\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mchunk\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mdict\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[32m   2733\u001b[39m \u001b[43m            \u001b[49m\u001b[38;5;129;43;01mand\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43m(\u001b[49m\u001b[43mints\u001b[49m\u001b[43m \u001b[49m\u001b[43m:=\u001b[49m\u001b[43m \u001b[49m\u001b[43mchunk\u001b[49m\u001b[43m.\u001b[49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[43mINTERRUPT\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01mis\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;129;43;01mnot\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\n\u001b[32m   2734\u001b[39m \u001b[43m        \u001b[49m\u001b[43m)\u001b[49m\u001b[43m:\u001b[49m\n",
            "\u001b[36mFile \u001b[39m\u001b[32m~/agents-0-to-1/.venv/lib/python3.13/site-packages/langgraph/pregel/__init__.py:2436\u001b[39m, in \u001b[36mPregel.stream\u001b[39m\u001b[34m(self, input, config, stream_mode, output_keys, interrupt_before, interrupt_after, checkpoint_during, debug, subgraphs)\u001b[39m\n\u001b[32m   2434\u001b[39m         \u001b[38;5;28;01mfor\u001b[39;00m task \u001b[38;5;129;01min\u001b[39;00m loop.match_cached_writes():\n\u001b[32m   2435\u001b[39m             loop.output_writes(task.id, task.writes, cached=\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[32m-> \u001b[39m\u001b[32m2436\u001b[39m \u001b[43m        \u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43m_\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mrunner\u001b[49m\u001b[43m.\u001b[49m\u001b[43mtick\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m   2437\u001b[39m \u001b[43m            \u001b[49m\u001b[43m[\u001b[49m\u001b[43mt\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mt\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mloop\u001b[49m\u001b[43m.\u001b[49m\u001b[43mtasks\u001b[49m\u001b[43m.\u001b[49m\u001b[43mvalues\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mif\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;129;43;01mnot\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mt\u001b[49m\u001b[43m.\u001b[49m\u001b[43mwrites\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   2438\u001b[39m \u001b[43m            \u001b[49m\u001b[43mtimeout\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mstep_timeout\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   2439\u001b[39m \u001b[43m            \u001b[49m\u001b[43mget_waiter\u001b[49m\u001b[43m=\u001b[49m\u001b[43mget_waiter\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   2440\u001b[39m \u001b[43m            \u001b[49m\u001b[43mschedule_task\u001b[49m\u001b[43m=\u001b[49m\u001b[43mloop\u001b[49m\u001b[43m.\u001b[49m\u001b[43maccept_push\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   2441\u001b[39m \u001b[43m        \u001b[49m\u001b[43m)\u001b[49m\u001b[43m:\u001b[49m\n\u001b[32m   2442\u001b[39m \u001b[43m            \u001b[49m\u001b[38;5;66;43;03m# emit output\u001b[39;49;00m\n\u001b[32m   2443\u001b[39m \u001b[43m            \u001b[49m\u001b[38;5;28;43;01myield from\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43moutput\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   2444\u001b[39m \u001b[38;5;66;03m# emit output\u001b[39;00m\n",
            "\u001b[36mFile \u001b[39m\u001b[32m~/agents-0-to-1/.venv/lib/python3.13/site-packages/langgraph/pregel/runner.py:161\u001b[39m, in \u001b[36mPregelRunner.tick\u001b[39m\u001b[34m(self, tasks, reraise, timeout, retry_policy, get_waiter, schedule_task)\u001b[39m\n\u001b[32m    159\u001b[39m t = tasks[\u001b[32m0\u001b[39m]\n\u001b[32m    160\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m161\u001b[39m     \u001b[43mrun_with_retry\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    162\u001b[39m \u001b[43m        \u001b[49m\u001b[43mt\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    163\u001b[39m \u001b[43m        \u001b[49m\u001b[43mretry_policy\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    164\u001b[39m \u001b[43m        \u001b[49m\u001b[43mconfigurable\u001b[49m\u001b[43m=\u001b[49m\u001b[43m{\u001b[49m\n\u001b[32m    165\u001b[39m \u001b[43m            \u001b[49m\u001b[43mCONFIG_KEY_CALL\u001b[49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mpartial\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    166\u001b[39m \u001b[43m                \u001b[49m\u001b[43m_call\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    167\u001b[39m \u001b[43m                \u001b[49m\u001b[43mweakref\u001b[49m\u001b[43m.\u001b[49m\u001b[43mref\u001b[49m\u001b[43m(\u001b[49m\u001b[43mt\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    168\u001b[39m \u001b[43m                \u001b[49m\u001b[43mretry\u001b[49m\u001b[43m=\u001b[49m\u001b[43mretry_policy\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    169\u001b[39m \u001b[43m                \u001b[49m\u001b[43mfutures\u001b[49m\u001b[43m=\u001b[49m\u001b[43mweakref\u001b[49m\u001b[43m.\u001b[49m\u001b[43mref\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfutures\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    170\u001b[39m \u001b[43m                \u001b[49m\u001b[43mschedule_task\u001b[49m\u001b[43m=\u001b[49m\u001b[43mschedule_task\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    171\u001b[39m \u001b[43m                \u001b[49m\u001b[43msubmit\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43msubmit\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    172\u001b[39m \u001b[43m            \u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    173\u001b[39m \u001b[43m        \u001b[49m\u001b[43m}\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    174\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    175\u001b[39m     \u001b[38;5;28mself\u001b[39m.commit(t, \u001b[38;5;28;01mNone\u001b[39;00m)\n\u001b[32m    176\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m exc:\n",
            "\u001b[36mFile \u001b[39m\u001b[32m~/agents-0-to-1/.venv/lib/python3.13/site-packages/langgraph/pregel/retry.py:40\u001b[39m, in \u001b[36mrun_with_retry\u001b[39m\u001b[34m(task, retry_policy, configurable)\u001b[39m\n\u001b[32m     38\u001b[39m     task.writes.clear()\n\u001b[32m     39\u001b[39m     \u001b[38;5;66;03m# run the task\u001b[39;00m\n\u001b[32m---> \u001b[39m\u001b[32m40\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mtask\u001b[49m\u001b[43m.\u001b[49m\u001b[43mproc\u001b[49m\u001b[43m.\u001b[49m\u001b[43minvoke\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtask\u001b[49m\u001b[43m.\u001b[49m\u001b[43minput\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mconfig\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     41\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m ParentCommand \u001b[38;5;28;01mas\u001b[39;00m exc:\n\u001b[32m     42\u001b[39m     ns: \u001b[38;5;28mstr\u001b[39m = config[CONF][CONFIG_KEY_CHECKPOINT_NS]\n",
            "\u001b[36mFile \u001b[39m\u001b[32m~/agents-0-to-1/.venv/lib/python3.13/site-packages/langgraph/utils/runnable.py:623\u001b[39m, in \u001b[36mRunnableSeq.invoke\u001b[39m\u001b[34m(self, input, config, **kwargs)\u001b[39m\n\u001b[32m    621\u001b[39m     \u001b[38;5;66;03m# run in context\u001b[39;00m\n\u001b[32m    622\u001b[39m     \u001b[38;5;28;01mwith\u001b[39;00m set_config_context(config, run) \u001b[38;5;28;01mas\u001b[39;00m context:\n\u001b[32m--> \u001b[39m\u001b[32m623\u001b[39m         \u001b[38;5;28minput\u001b[39m = \u001b[43mcontext\u001b[49m\u001b[43m.\u001b[49m\u001b[43mrun\u001b[49m\u001b[43m(\u001b[49m\u001b[43mstep\u001b[49m\u001b[43m.\u001b[49m\u001b[43minvoke\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mconfig\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    624\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m    625\u001b[39m     \u001b[38;5;28minput\u001b[39m = step.invoke(\u001b[38;5;28minput\u001b[39m, config)\n",
            "\u001b[36mFile \u001b[39m\u001b[32m~/agents-0-to-1/.venv/lib/python3.13/site-packages/langgraph/utils/runnable.py:377\u001b[39m, in \u001b[36mRunnableCallable.invoke\u001b[39m\u001b[34m(self, input, config, **kwargs)\u001b[39m\n\u001b[32m    375\u001b[39m         run_manager.on_chain_end(ret)\n\u001b[32m    376\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m377\u001b[39m     ret = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    378\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m.recurse \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(ret, Runnable):\n\u001b[32m    379\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m ret.invoke(\u001b[38;5;28minput\u001b[39m, config)\n",
            "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[30]\u001b[39m\u001b[32m, line 68\u001b[39m, in \u001b[36mresponse_generator_node\u001b[39m\u001b[34m(state)\u001b[39m\n\u001b[32m     57\u001b[39m         context = \u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mSummary: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mstate[\u001b[33m'\u001b[39m\u001b[33msummary\u001b[39m\u001b[33m'\u001b[39m]\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[33mUser: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mlast_msg.content\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m\"\u001b[39m\n\u001b[32m     59\u001b[39m         prompt = PromptTemplate(\n\u001b[32m     60\u001b[39m             input_variables=[\u001b[33m\"\u001b[39m\u001b[33mcontext\u001b[39m\u001b[33m\"\u001b[39m],\n\u001b[32m     61\u001b[39m             template=\u001b[33m\"\"\"\u001b[39m\u001b[33mBased on the conversation context, provide a helpful response.\u001b[39m\n\u001b[32m   (...)\u001b[39m\u001b[32m     65\u001b[39m \u001b[33mResponse:\u001b[39m\u001b[33m\"\"\"\u001b[39m\n\u001b[32m     66\u001b[39m         )\n\u001b[32m---> \u001b[39m\u001b[32m68\u001b[39m         response = \u001b[43mllm\u001b[49m\u001b[43m.\u001b[49m\u001b[43minvoke\u001b[49m\u001b[43m(\u001b[49m\u001b[43mprompt\u001b[49m\u001b[43m.\u001b[49m\u001b[43mformat\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcontext\u001b[49m\u001b[43m=\u001b[49m\u001b[43mcontext\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     70\u001b[39m         \u001b[38;5;28mprint\u001b[39m(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33m[Response Generator] Generated response\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m     72\u001b[39m         \u001b[38;5;28;01mreturn\u001b[39;00m {\n\u001b[32m     73\u001b[39m             \u001b[33m\"\u001b[39m\u001b[33mmessages\u001b[39m\u001b[33m\"\u001b[39m: [AIMessage(content=response.content)],\n\u001b[32m     74\u001b[39m             \u001b[33m\"\u001b[39m\u001b[33msummary\u001b[39m\u001b[33m\"\u001b[39m: state[\u001b[33m\"\u001b[39m\u001b[33msummary\u001b[39m\u001b[33m\"\u001b[39m],\n\u001b[32m     75\u001b[39m             \u001b[33m\"\u001b[39m\u001b[33mwindow_size\u001b[39m\u001b[33m\"\u001b[39m: state[\u001b[33m\"\u001b[39m\u001b[33mwindow_size\u001b[39m\u001b[33m\"\u001b[39m]\n\u001b[32m     76\u001b[39m         }\n",
            "\u001b[36mFile \u001b[39m\u001b[32m~/agents-0-to-1/.venv/lib/python3.13/site-packages/langchain_core/language_models/chat_models.py:372\u001b[39m, in \u001b[36mBaseChatModel.invoke\u001b[39m\u001b[34m(self, input, config, stop, **kwargs)\u001b[39m\n\u001b[32m    360\u001b[39m \u001b[38;5;129m@override\u001b[39m\n\u001b[32m    361\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34minvoke\u001b[39m(\n\u001b[32m    362\u001b[39m     \u001b[38;5;28mself\u001b[39m,\n\u001b[32m   (...)\u001b[39m\u001b[32m    367\u001b[39m     **kwargs: Any,\n\u001b[32m    368\u001b[39m ) -> BaseMessage:\n\u001b[32m    369\u001b[39m     config = ensure_config(config)\n\u001b[32m    370\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m cast(\n\u001b[32m    371\u001b[39m         \u001b[33m\"\u001b[39m\u001b[33mChatGeneration\u001b[39m\u001b[33m\"\u001b[39m,\n\u001b[32m--> \u001b[39m\u001b[32m372\u001b[39m         \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mgenerate_prompt\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    373\u001b[39m \u001b[43m            \u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_convert_input\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    374\u001b[39m \u001b[43m            \u001b[49m\u001b[43mstop\u001b[49m\u001b[43m=\u001b[49m\u001b[43mstop\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    375\u001b[39m \u001b[43m            \u001b[49m\u001b[43mcallbacks\u001b[49m\u001b[43m=\u001b[49m\u001b[43mconfig\u001b[49m\u001b[43m.\u001b[49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mcallbacks\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    376\u001b[39m \u001b[43m            \u001b[49m\u001b[43mtags\u001b[49m\u001b[43m=\u001b[49m\u001b[43mconfig\u001b[49m\u001b[43m.\u001b[49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mtags\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    377\u001b[39m \u001b[43m            \u001b[49m\u001b[43mmetadata\u001b[49m\u001b[43m=\u001b[49m\u001b[43mconfig\u001b[49m\u001b[43m.\u001b[49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mmetadata\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    378\u001b[39m \u001b[43m            \u001b[49m\u001b[43mrun_name\u001b[49m\u001b[43m=\u001b[49m\u001b[43mconfig\u001b[49m\u001b[43m.\u001b[49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mrun_name\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    379\u001b[39m \u001b[43m            \u001b[49m\u001b[43mrun_id\u001b[49m\u001b[43m=\u001b[49m\u001b[43mconfig\u001b[49m\u001b[43m.\u001b[49m\u001b[43mpop\u001b[49m\u001b[43m(\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mrun_id\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    380\u001b[39m \u001b[43m            \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    381\u001b[39m \u001b[43m        \u001b[49m\u001b[43m)\u001b[49m.generations[\u001b[32m0\u001b[39m][\u001b[32m0\u001b[39m],\n\u001b[32m    382\u001b[39m     ).message\n",
            "\u001b[36mFile \u001b[39m\u001b[32m~/agents-0-to-1/.venv/lib/python3.13/site-packages/langchain_core/language_models/chat_models.py:957\u001b[39m, in \u001b[36mBaseChatModel.generate_prompt\u001b[39m\u001b[34m(self, prompts, stop, callbacks, **kwargs)\u001b[39m\n\u001b[32m    948\u001b[39m \u001b[38;5;129m@override\u001b[39m\n\u001b[32m    949\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mgenerate_prompt\u001b[39m(\n\u001b[32m    950\u001b[39m     \u001b[38;5;28mself\u001b[39m,\n\u001b[32m   (...)\u001b[39m\u001b[32m    954\u001b[39m     **kwargs: Any,\n\u001b[32m    955\u001b[39m ) -> LLMResult:\n\u001b[32m    956\u001b[39m     prompt_messages = [p.to_messages() \u001b[38;5;28;01mfor\u001b[39;00m p \u001b[38;5;129;01min\u001b[39;00m prompts]\n\u001b[32m--> \u001b[39m\u001b[32m957\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mgenerate\u001b[49m\u001b[43m(\u001b[49m\u001b[43mprompt_messages\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mstop\u001b[49m\u001b[43m=\u001b[49m\u001b[43mstop\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcallbacks\u001b[49m\u001b[43m=\u001b[49m\u001b[43mcallbacks\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
            "\u001b[36mFile \u001b[39m\u001b[32m~/agents-0-to-1/.venv/lib/python3.13/site-packages/langchain_core/language_models/chat_models.py:776\u001b[39m, in \u001b[36mBaseChatModel.generate\u001b[39m\u001b[34m(self, messages, stop, callbacks, tags, metadata, run_name, run_id, **kwargs)\u001b[39m\n\u001b[32m    773\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m i, m \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28menumerate\u001b[39m(input_messages):\n\u001b[32m    774\u001b[39m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m    775\u001b[39m         results.append(\n\u001b[32m--> \u001b[39m\u001b[32m776\u001b[39m             \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_generate_with_cache\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    777\u001b[39m \u001b[43m                \u001b[49m\u001b[43mm\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    778\u001b[39m \u001b[43m                \u001b[49m\u001b[43mstop\u001b[49m\u001b[43m=\u001b[49m\u001b[43mstop\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    779\u001b[39m \u001b[43m                \u001b[49m\u001b[43mrun_manager\u001b[49m\u001b[43m=\u001b[49m\u001b[43mrun_managers\u001b[49m\u001b[43m[\u001b[49m\u001b[43mi\u001b[49m\u001b[43m]\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mif\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mrun_managers\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01melse\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[32m    780\u001b[39m \u001b[43m                \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    781\u001b[39m \u001b[43m            \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    782\u001b[39m         )\n\u001b[32m    783\u001b[39m     \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mBaseException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[32m    784\u001b[39m         \u001b[38;5;28;01mif\u001b[39;00m run_managers:\n",
            "\u001b[36mFile \u001b[39m\u001b[32m~/agents-0-to-1/.venv/lib/python3.13/site-packages/langchain_core/language_models/chat_models.py:1022\u001b[39m, in \u001b[36mBaseChatModel._generate_with_cache\u001b[39m\u001b[34m(self, messages, stop, run_manager, **kwargs)\u001b[39m\n\u001b[32m   1020\u001b[39m     result = generate_from_stream(\u001b[38;5;28miter\u001b[39m(chunks))\n\u001b[32m   1021\u001b[39m \u001b[38;5;28;01melif\u001b[39;00m inspect.signature(\u001b[38;5;28mself\u001b[39m._generate).parameters.get(\u001b[33m\"\u001b[39m\u001b[33mrun_manager\u001b[39m\u001b[33m\"\u001b[39m):\n\u001b[32m-> \u001b[39m\u001b[32m1022\u001b[39m     result = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_generate\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m   1023\u001b[39m \u001b[43m        \u001b[49m\u001b[43mmessages\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mstop\u001b[49m\u001b[43m=\u001b[49m\u001b[43mstop\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mrun_manager\u001b[49m\u001b[43m=\u001b[49m\u001b[43mrun_manager\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\n\u001b[32m   1024\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1025\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m   1026\u001b[39m     result = \u001b[38;5;28mself\u001b[39m._generate(messages, stop=stop, **kwargs)\n",
            "\u001b[36mFile \u001b[39m\u001b[32m~/agents-0-to-1/.venv/lib/python3.13/site-packages/langchain_openai/chat_models/base.py:1071\u001b[39m, in \u001b[36mBaseChatOpenAI._generate\u001b[39m\u001b[34m(self, messages, stop, run_manager, **kwargs)\u001b[39m\n\u001b[32m   1069\u001b[39m     generation_info = {\u001b[33m\"\u001b[39m\u001b[33mheaders\u001b[39m\u001b[33m\"\u001b[39m: \u001b[38;5;28mdict\u001b[39m(raw_response.headers)}\n\u001b[32m   1070\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m1071\u001b[39m     response = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mclient\u001b[49m\u001b[43m.\u001b[49m\u001b[43mcreate\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mpayload\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1072\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m._create_chat_result(response, generation_info)\n",
            "\u001b[36mFile \u001b[39m\u001b[32m~/agents-0-to-1/.venv/lib/python3.13/site-packages/openai/_utils/_utils.py:287\u001b[39m, in \u001b[36mrequired_args.<locals>.inner.<locals>.wrapper\u001b[39m\u001b[34m(*args, **kwargs)\u001b[39m\n\u001b[32m    285\u001b[39m             msg = \u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mMissing required argument: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mquote(missing[\u001b[32m0\u001b[39m])\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m\"\u001b[39m\n\u001b[32m    286\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mTypeError\u001b[39;00m(msg)\n\u001b[32m--> \u001b[39m\u001b[32m287\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
            "\u001b[36mFile \u001b[39m\u001b[32m~/agents-0-to-1/.venv/lib/python3.13/site-packages/openai/resources/chat/completions/completions.py:925\u001b[39m, in \u001b[36mCompletions.create\u001b[39m\u001b[34m(self, messages, model, audio, frequency_penalty, function_call, functions, logit_bias, logprobs, max_completion_tokens, max_tokens, metadata, modalities, n, parallel_tool_calls, prediction, presence_penalty, reasoning_effort, response_format, seed, service_tier, stop, store, stream, stream_options, temperature, tool_choice, tools, top_logprobs, top_p, user, web_search_options, extra_headers, extra_query, extra_body, timeout)\u001b[39m\n\u001b[32m    882\u001b[39m \u001b[38;5;129m@required_args\u001b[39m([\u001b[33m\"\u001b[39m\u001b[33mmessages\u001b[39m\u001b[33m\"\u001b[39m, \u001b[33m\"\u001b[39m\u001b[33mmodel\u001b[39m\u001b[33m\"\u001b[39m], [\u001b[33m\"\u001b[39m\u001b[33mmessages\u001b[39m\u001b[33m\"\u001b[39m, \u001b[33m\"\u001b[39m\u001b[33mmodel\u001b[39m\u001b[33m\"\u001b[39m, \u001b[33m\"\u001b[39m\u001b[33mstream\u001b[39m\u001b[33m\"\u001b[39m])\n\u001b[32m    883\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mcreate\u001b[39m(\n\u001b[32m    884\u001b[39m     \u001b[38;5;28mself\u001b[39m,\n\u001b[32m   (...)\u001b[39m\u001b[32m    922\u001b[39m     timeout: \u001b[38;5;28mfloat\u001b[39m | httpx.Timeout | \u001b[38;5;28;01mNone\u001b[39;00m | NotGiven = NOT_GIVEN,\n\u001b[32m    923\u001b[39m ) -> ChatCompletion | Stream[ChatCompletionChunk]:\n\u001b[32m    924\u001b[39m     validate_response_format(response_format)\n\u001b[32m--> \u001b[39m\u001b[32m925\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_post\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    926\u001b[39m \u001b[43m        \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43m/chat/completions\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[32m    927\u001b[39m \u001b[43m        \u001b[49m\u001b[43mbody\u001b[49m\u001b[43m=\u001b[49m\u001b[43mmaybe_transform\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    928\u001b[39m \u001b[43m            \u001b[49m\u001b[43m{\u001b[49m\n\u001b[32m    929\u001b[39m \u001b[43m                \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mmessages\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mmessages\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    930\u001b[39m \u001b[43m                \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mmodel\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    931\u001b[39m \u001b[43m                \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43maudio\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43maudio\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    932\u001b[39m \u001b[43m                \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mfrequency_penalty\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mfrequency_penalty\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    933\u001b[39m \u001b[43m                \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mfunction_call\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mfunction_call\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    934\u001b[39m \u001b[43m                \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mfunctions\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mfunctions\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    935\u001b[39m \u001b[43m                \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mlogit_bias\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mlogit_bias\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    936\u001b[39m \u001b[43m                \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mlogprobs\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mlogprobs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    937\u001b[39m \u001b[43m                \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mmax_completion_tokens\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mmax_completion_tokens\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    938\u001b[39m \u001b[43m                \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mmax_tokens\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mmax_tokens\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    939\u001b[39m \u001b[43m                \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mmetadata\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mmetadata\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    940\u001b[39m \u001b[43m                \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mmodalities\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mmodalities\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    941\u001b[39m \u001b[43m                \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mn\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mn\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    942\u001b[39m \u001b[43m                \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mparallel_tool_calls\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mparallel_tool_calls\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    943\u001b[39m \u001b[43m                \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mprediction\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mprediction\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    944\u001b[39m \u001b[43m                \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mpresence_penalty\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mpresence_penalty\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    945\u001b[39m \u001b[43m                \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mreasoning_effort\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mreasoning_effort\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    946\u001b[39m \u001b[43m                \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mresponse_format\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mresponse_format\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    947\u001b[39m \u001b[43m                \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mseed\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mseed\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    948\u001b[39m \u001b[43m                \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mservice_tier\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mservice_tier\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    949\u001b[39m \u001b[43m                \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mstop\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mstop\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    950\u001b[39m \u001b[43m                \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mstore\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mstore\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    951\u001b[39m \u001b[43m                \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mstream\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mstream\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    952\u001b[39m \u001b[43m                \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mstream_options\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mstream_options\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    953\u001b[39m \u001b[43m                \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mtemperature\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mtemperature\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    954\u001b[39m \u001b[43m                \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mtool_choice\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mtool_choice\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    955\u001b[39m \u001b[43m                \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mtools\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mtools\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    956\u001b[39m \u001b[43m                \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mtop_logprobs\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mtop_logprobs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    957\u001b[39m \u001b[43m                \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mtop_p\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mtop_p\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    958\u001b[39m \u001b[43m                \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43muser\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43muser\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    959\u001b[39m \u001b[43m                \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mweb_search_options\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mweb_search_options\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    960\u001b[39m \u001b[43m            \u001b[49m\u001b[43m}\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    961\u001b[39m \u001b[43m            \u001b[49m\u001b[43mcompletion_create_params\u001b[49m\u001b[43m.\u001b[49m\u001b[43mCompletionCreateParamsStreaming\u001b[49m\n\u001b[32m    962\u001b[39m \u001b[43m            \u001b[49m\u001b[38;5;28;43;01mif\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mstream\u001b[49m\n\u001b[32m    963\u001b[39m \u001b[43m            \u001b[49m\u001b[38;5;28;43;01melse\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mcompletion_create_params\u001b[49m\u001b[43m.\u001b[49m\u001b[43mCompletionCreateParamsNonStreaming\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    964\u001b[39m \u001b[43m        \u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    965\u001b[39m \u001b[43m        \u001b[49m\u001b[43moptions\u001b[49m\u001b[43m=\u001b[49m\u001b[43mmake_request_options\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    966\u001b[39m \u001b[43m            \u001b[49m\u001b[43mextra_headers\u001b[49m\u001b[43m=\u001b[49m\u001b[43mextra_headers\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mextra_query\u001b[49m\u001b[43m=\u001b[49m\u001b[43mextra_query\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mextra_body\u001b[49m\u001b[43m=\u001b[49m\u001b[43mextra_body\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtimeout\u001b[49m\u001b[43m=\u001b[49m\u001b[43mtimeout\u001b[49m\n\u001b[32m    967\u001b[39m \u001b[43m        \u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    968\u001b[39m \u001b[43m        \u001b[49m\u001b[43mcast_to\u001b[49m\u001b[43m=\u001b[49m\u001b[43mChatCompletion\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    969\u001b[39m \u001b[43m        \u001b[49m\u001b[43mstream\u001b[49m\u001b[43m=\u001b[49m\u001b[43mstream\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01mor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[32m    970\u001b[39m \u001b[43m        \u001b[49m\u001b[43mstream_cls\u001b[49m\u001b[43m=\u001b[49m\u001b[43mStream\u001b[49m\u001b[43m[\u001b[49m\u001b[43mChatCompletionChunk\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    971\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n",
            "\u001b[36mFile \u001b[39m\u001b[32m~/agents-0-to-1/.venv/lib/python3.13/site-packages/openai/_base_client.py:1249\u001b[39m, in \u001b[36mSyncAPIClient.post\u001b[39m\u001b[34m(self, path, cast_to, body, options, files, stream, stream_cls)\u001b[39m\n\u001b[32m   1235\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mpost\u001b[39m(\n\u001b[32m   1236\u001b[39m     \u001b[38;5;28mself\u001b[39m,\n\u001b[32m   1237\u001b[39m     path: \u001b[38;5;28mstr\u001b[39m,\n\u001b[32m   (...)\u001b[39m\u001b[32m   1244\u001b[39m     stream_cls: \u001b[38;5;28mtype\u001b[39m[_StreamT] | \u001b[38;5;28;01mNone\u001b[39;00m = \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[32m   1245\u001b[39m ) -> ResponseT | _StreamT:\n\u001b[32m   1246\u001b[39m     opts = FinalRequestOptions.construct(\n\u001b[32m   1247\u001b[39m         method=\u001b[33m\"\u001b[39m\u001b[33mpost\u001b[39m\u001b[33m\"\u001b[39m, url=path, json_data=body, files=to_httpx_files(files), **options\n\u001b[32m   1248\u001b[39m     )\n\u001b[32m-> \u001b[39m\u001b[32m1249\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m cast(ResponseT, \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mrequest\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcast_to\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mopts\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mstream\u001b[49m\u001b[43m=\u001b[49m\u001b[43mstream\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mstream_cls\u001b[49m\u001b[43m=\u001b[49m\u001b[43mstream_cls\u001b[49m\u001b[43m)\u001b[49m)\n",
            "\u001b[36mFile \u001b[39m\u001b[32m~/agents-0-to-1/.venv/lib/python3.13/site-packages/openai/_base_client.py:972\u001b[39m, in \u001b[36mSyncAPIClient.request\u001b[39m\u001b[34m(self, cast_to, options, stream, stream_cls)\u001b[39m\n\u001b[32m    970\u001b[39m response = \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m    971\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m972\u001b[39m     response = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_client\u001b[49m\u001b[43m.\u001b[49m\u001b[43msend\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    973\u001b[39m \u001b[43m        \u001b[49m\u001b[43mrequest\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    974\u001b[39m \u001b[43m        \u001b[49m\u001b[43mstream\u001b[49m\u001b[43m=\u001b[49m\u001b[43mstream\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01mor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_should_stream_response_body\u001b[49m\u001b[43m(\u001b[49m\u001b[43mrequest\u001b[49m\u001b[43m=\u001b[49m\u001b[43mrequest\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    975\u001b[39m \u001b[43m        \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    976\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    977\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m httpx.TimeoutException \u001b[38;5;28;01mas\u001b[39;00m err:\n\u001b[32m    978\u001b[39m     log.debug(\u001b[33m\"\u001b[39m\u001b[33mEncountered httpx.TimeoutException\u001b[39m\u001b[33m\"\u001b[39m, exc_info=\u001b[38;5;28;01mTrue\u001b[39;00m)\n",
            "\u001b[36mFile \u001b[39m\u001b[32m~/agents-0-to-1/.venv/lib/python3.13/site-packages/httpx/_client.py:914\u001b[39m, in \u001b[36mClient.send\u001b[39m\u001b[34m(self, request, stream, auth, follow_redirects)\u001b[39m\n\u001b[32m    910\u001b[39m \u001b[38;5;28mself\u001b[39m._set_timeout(request)\n\u001b[32m    912\u001b[39m auth = \u001b[38;5;28mself\u001b[39m._build_request_auth(request, auth)\n\u001b[32m--> \u001b[39m\u001b[32m914\u001b[39m response = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_send_handling_auth\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    915\u001b[39m \u001b[43m    \u001b[49m\u001b[43mrequest\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    916\u001b[39m \u001b[43m    \u001b[49m\u001b[43mauth\u001b[49m\u001b[43m=\u001b[49m\u001b[43mauth\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    917\u001b[39m \u001b[43m    \u001b[49m\u001b[43mfollow_redirects\u001b[49m\u001b[43m=\u001b[49m\u001b[43mfollow_redirects\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    918\u001b[39m \u001b[43m    \u001b[49m\u001b[43mhistory\u001b[49m\u001b[43m=\u001b[49m\u001b[43m[\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    919\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    920\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m    921\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m stream:\n",
            "\u001b[36mFile \u001b[39m\u001b[32m~/agents-0-to-1/.venv/lib/python3.13/site-packages/httpx/_client.py:942\u001b[39m, in \u001b[36mClient._send_handling_auth\u001b[39m\u001b[34m(self, request, auth, follow_redirects, history)\u001b[39m\n\u001b[32m    939\u001b[39m request = \u001b[38;5;28mnext\u001b[39m(auth_flow)\n\u001b[32m    941\u001b[39m \u001b[38;5;28;01mwhile\u001b[39;00m \u001b[38;5;28;01mTrue\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m942\u001b[39m     response = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_send_handling_redirects\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    943\u001b[39m \u001b[43m        \u001b[49m\u001b[43mrequest\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    944\u001b[39m \u001b[43m        \u001b[49m\u001b[43mfollow_redirects\u001b[49m\u001b[43m=\u001b[49m\u001b[43mfollow_redirects\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    945\u001b[39m \u001b[43m        \u001b[49m\u001b[43mhistory\u001b[49m\u001b[43m=\u001b[49m\u001b[43mhistory\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    946\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    947\u001b[39m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m    948\u001b[39m         \u001b[38;5;28;01mtry\u001b[39;00m:\n",
            "\u001b[36mFile \u001b[39m\u001b[32m~/agents-0-to-1/.venv/lib/python3.13/site-packages/httpx/_client.py:979\u001b[39m, in \u001b[36mClient._send_handling_redirects\u001b[39m\u001b[34m(self, request, follow_redirects, history)\u001b[39m\n\u001b[32m    976\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m hook \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m._event_hooks[\u001b[33m\"\u001b[39m\u001b[33mrequest\u001b[39m\u001b[33m\"\u001b[39m]:\n\u001b[32m    977\u001b[39m     hook(request)\n\u001b[32m--> \u001b[39m\u001b[32m979\u001b[39m response = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_send_single_request\u001b[49m\u001b[43m(\u001b[49m\u001b[43mrequest\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    980\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m    981\u001b[39m     \u001b[38;5;28;01mfor\u001b[39;00m hook \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m._event_hooks[\u001b[33m\"\u001b[39m\u001b[33mresponse\u001b[39m\u001b[33m\"\u001b[39m]:\n",
            "\u001b[36mFile \u001b[39m\u001b[32m~/agents-0-to-1/.venv/lib/python3.13/site-packages/httpx/_client.py:1014\u001b[39m, in \u001b[36mClient._send_single_request\u001b[39m\u001b[34m(self, request)\u001b[39m\n\u001b[32m   1009\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mRuntimeError\u001b[39;00m(\n\u001b[32m   1010\u001b[39m         \u001b[33m\"\u001b[39m\u001b[33mAttempted to send an async request with a sync Client instance.\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m   1011\u001b[39m     )\n\u001b[32m   1013\u001b[39m \u001b[38;5;28;01mwith\u001b[39;00m request_context(request=request):\n\u001b[32m-> \u001b[39m\u001b[32m1014\u001b[39m     response = \u001b[43mtransport\u001b[49m\u001b[43m.\u001b[49m\u001b[43mhandle_request\u001b[49m\u001b[43m(\u001b[49m\u001b[43mrequest\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1016\u001b[39m \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(response.stream, SyncByteStream)\n\u001b[32m   1018\u001b[39m response.request = request\n",
            "\u001b[36mFile \u001b[39m\u001b[32m~/agents-0-to-1/.venv/lib/python3.13/site-packages/httpx/_transports/default.py:250\u001b[39m, in \u001b[36mHTTPTransport.handle_request\u001b[39m\u001b[34m(self, request)\u001b[39m\n\u001b[32m    237\u001b[39m req = httpcore.Request(\n\u001b[32m    238\u001b[39m     method=request.method,\n\u001b[32m    239\u001b[39m     url=httpcore.URL(\n\u001b[32m   (...)\u001b[39m\u001b[32m    247\u001b[39m     extensions=request.extensions,\n\u001b[32m    248\u001b[39m )\n\u001b[32m    249\u001b[39m \u001b[38;5;28;01mwith\u001b[39;00m map_httpcore_exceptions():\n\u001b[32m--> \u001b[39m\u001b[32m250\u001b[39m     resp = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_pool\u001b[49m\u001b[43m.\u001b[49m\u001b[43mhandle_request\u001b[49m\u001b[43m(\u001b[49m\u001b[43mreq\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    252\u001b[39m \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(resp.stream, typing.Iterable)\n\u001b[32m    254\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m Response(\n\u001b[32m    255\u001b[39m     status_code=resp.status,\n\u001b[32m    256\u001b[39m     headers=resp.headers,\n\u001b[32m    257\u001b[39m     stream=ResponseStream(resp.stream),\n\u001b[32m    258\u001b[39m     extensions=resp.extensions,\n\u001b[32m    259\u001b[39m )\n",
            "\u001b[36mFile \u001b[39m\u001b[32m~/agents-0-to-1/.venv/lib/python3.13/site-packages/httpcore/_sync/connection_pool.py:256\u001b[39m, in \u001b[36mConnectionPool.handle_request\u001b[39m\u001b[34m(self, request)\u001b[39m\n\u001b[32m    253\u001b[39m         closing = \u001b[38;5;28mself\u001b[39m._assign_requests_to_connections()\n\u001b[32m    255\u001b[39m     \u001b[38;5;28mself\u001b[39m._close_connections(closing)\n\u001b[32m--> \u001b[39m\u001b[32m256\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m exc \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m    258\u001b[39m \u001b[38;5;66;03m# Return the response. Note that in this case we still have to manage\u001b[39;00m\n\u001b[32m    259\u001b[39m \u001b[38;5;66;03m# the point at which the response is closed.\u001b[39;00m\n\u001b[32m    260\u001b[39m \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(response.stream, typing.Iterable)\n",
            "\u001b[36mFile \u001b[39m\u001b[32m~/agents-0-to-1/.venv/lib/python3.13/site-packages/httpcore/_sync/connection_pool.py:236\u001b[39m, in \u001b[36mConnectionPool.handle_request\u001b[39m\u001b[34m(self, request)\u001b[39m\n\u001b[32m    232\u001b[39m connection = pool_request.wait_for_connection(timeout=timeout)\n\u001b[32m    234\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m    235\u001b[39m     \u001b[38;5;66;03m# Send the request on the assigned connection.\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m236\u001b[39m     response = \u001b[43mconnection\u001b[49m\u001b[43m.\u001b[49m\u001b[43mhandle_request\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    237\u001b[39m \u001b[43m        \u001b[49m\u001b[43mpool_request\u001b[49m\u001b[43m.\u001b[49m\u001b[43mrequest\u001b[49m\n\u001b[32m    238\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    239\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m ConnectionNotAvailable:\n\u001b[32m    240\u001b[39m     \u001b[38;5;66;03m# In some cases a connection may initially be available to\u001b[39;00m\n\u001b[32m    241\u001b[39m     \u001b[38;5;66;03m# handle a request, but then become unavailable.\u001b[39;00m\n\u001b[32m    242\u001b[39m     \u001b[38;5;66;03m#\u001b[39;00m\n\u001b[32m    243\u001b[39m     \u001b[38;5;66;03m# In this case we clear the connection and try again.\u001b[39;00m\n\u001b[32m    244\u001b[39m     pool_request.clear_connection()\n",
            "\u001b[36mFile \u001b[39m\u001b[32m~/agents-0-to-1/.venv/lib/python3.13/site-packages/httpcore/_sync/connection.py:103\u001b[39m, in \u001b[36mHTTPConnection.handle_request\u001b[39m\u001b[34m(self, request)\u001b[39m\n\u001b[32m    100\u001b[39m     \u001b[38;5;28mself\u001b[39m._connect_failed = \u001b[38;5;28;01mTrue\u001b[39;00m\n\u001b[32m    101\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m exc\n\u001b[32m--> \u001b[39m\u001b[32m103\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_connection\u001b[49m\u001b[43m.\u001b[49m\u001b[43mhandle_request\u001b[49m\u001b[43m(\u001b[49m\u001b[43mrequest\u001b[49m\u001b[43m)\u001b[49m\n",
            "\u001b[36mFile \u001b[39m\u001b[32m~/agents-0-to-1/.venv/lib/python3.13/site-packages/httpcore/_sync/http11.py:136\u001b[39m, in \u001b[36mHTTP11Connection.handle_request\u001b[39m\u001b[34m(self, request)\u001b[39m\n\u001b[32m    134\u001b[39m     \u001b[38;5;28;01mwith\u001b[39;00m Trace(\u001b[33m\"\u001b[39m\u001b[33mresponse_closed\u001b[39m\u001b[33m\"\u001b[39m, logger, request) \u001b[38;5;28;01mas\u001b[39;00m trace:\n\u001b[32m    135\u001b[39m         \u001b[38;5;28mself\u001b[39m._response_closed()\n\u001b[32m--> \u001b[39m\u001b[32m136\u001b[39m \u001b[38;5;28;01mraise\u001b[39;00m exc\n",
            "\u001b[36mFile \u001b[39m\u001b[32m~/agents-0-to-1/.venv/lib/python3.13/site-packages/httpcore/_sync/http11.py:106\u001b[39m, in \u001b[36mHTTP11Connection.handle_request\u001b[39m\u001b[34m(self, request)\u001b[39m\n\u001b[32m     95\u001b[39m     \u001b[38;5;28;01mpass\u001b[39;00m\n\u001b[32m     97\u001b[39m \u001b[38;5;28;01mwith\u001b[39;00m Trace(\n\u001b[32m     98\u001b[39m     \u001b[33m\"\u001b[39m\u001b[33mreceive_response_headers\u001b[39m\u001b[33m\"\u001b[39m, logger, request, kwargs\n\u001b[32m     99\u001b[39m ) \u001b[38;5;28;01mas\u001b[39;00m trace:\n\u001b[32m    100\u001b[39m     (\n\u001b[32m    101\u001b[39m         http_version,\n\u001b[32m    102\u001b[39m         status,\n\u001b[32m    103\u001b[39m         reason_phrase,\n\u001b[32m    104\u001b[39m         headers,\n\u001b[32m    105\u001b[39m         trailing_data,\n\u001b[32m--> \u001b[39m\u001b[32m106\u001b[39m     ) = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_receive_response_headers\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    107\u001b[39m     trace.return_value = (\n\u001b[32m    108\u001b[39m         http_version,\n\u001b[32m    109\u001b[39m         status,\n\u001b[32m    110\u001b[39m         reason_phrase,\n\u001b[32m    111\u001b[39m         headers,\n\u001b[32m    112\u001b[39m     )\n\u001b[32m    114\u001b[39m network_stream = \u001b[38;5;28mself\u001b[39m._network_stream\n",
            "\u001b[36mFile \u001b[39m\u001b[32m~/agents-0-to-1/.venv/lib/python3.13/site-packages/httpcore/_sync/http11.py:177\u001b[39m, in \u001b[36mHTTP11Connection._receive_response_headers\u001b[39m\u001b[34m(self, request)\u001b[39m\n\u001b[32m    174\u001b[39m timeout = timeouts.get(\u001b[33m\"\u001b[39m\u001b[33mread\u001b[39m\u001b[33m\"\u001b[39m, \u001b[38;5;28;01mNone\u001b[39;00m)\n\u001b[32m    176\u001b[39m \u001b[38;5;28;01mwhile\u001b[39;00m \u001b[38;5;28;01mTrue\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m177\u001b[39m     event = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_receive_event\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtimeout\u001b[49m\u001b[43m=\u001b[49m\u001b[43mtimeout\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    178\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(event, h11.Response):\n\u001b[32m    179\u001b[39m         \u001b[38;5;28;01mbreak\u001b[39;00m\n",
            "\u001b[36mFile \u001b[39m\u001b[32m~/agents-0-to-1/.venv/lib/python3.13/site-packages/httpcore/_sync/http11.py:217\u001b[39m, in \u001b[36mHTTP11Connection._receive_event\u001b[39m\u001b[34m(self, timeout)\u001b[39m\n\u001b[32m    214\u001b[39m     event = \u001b[38;5;28mself\u001b[39m._h11_state.next_event()\n\u001b[32m    216\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m event \u001b[38;5;129;01mis\u001b[39;00m h11.NEED_DATA:\n\u001b[32m--> \u001b[39m\u001b[32m217\u001b[39m     data = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_network_stream\u001b[49m\u001b[43m.\u001b[49m\u001b[43mread\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    218\u001b[39m \u001b[43m        \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mREAD_NUM_BYTES\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtimeout\u001b[49m\u001b[43m=\u001b[49m\u001b[43mtimeout\u001b[49m\n\u001b[32m    219\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    221\u001b[39m     \u001b[38;5;66;03m# If we feed this case through h11 we'll raise an exception like:\u001b[39;00m\n\u001b[32m    222\u001b[39m     \u001b[38;5;66;03m#\u001b[39;00m\n\u001b[32m    223\u001b[39m     \u001b[38;5;66;03m#     httpcore.RemoteProtocolError: can't handle event type\u001b[39;00m\n\u001b[32m   (...)\u001b[39m\u001b[32m    227\u001b[39m     \u001b[38;5;66;03m# perspective. Instead we handle this case distinctly and treat\u001b[39;00m\n\u001b[32m    228\u001b[39m     \u001b[38;5;66;03m# it as a ConnectError.\u001b[39;00m\n\u001b[32m    229\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m data == \u001b[33mb\u001b[39m\u001b[33m\"\u001b[39m\u001b[33m\"\u001b[39m \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mself\u001b[39m._h11_state.their_state == h11.SEND_RESPONSE:\n",
            "\u001b[36mFile \u001b[39m\u001b[32m~/agents-0-to-1/.venv/lib/python3.13/site-packages/httpcore/_backends/sync.py:128\u001b[39m, in \u001b[36mSyncStream.read\u001b[39m\u001b[34m(self, max_bytes, timeout)\u001b[39m\n\u001b[32m    126\u001b[39m \u001b[38;5;28;01mwith\u001b[39;00m map_exceptions(exc_map):\n\u001b[32m    127\u001b[39m     \u001b[38;5;28mself\u001b[39m._sock.settimeout(timeout)\n\u001b[32m--> \u001b[39m\u001b[32m128\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_sock\u001b[49m\u001b[43m.\u001b[49m\u001b[43mrecv\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmax_bytes\u001b[49m\u001b[43m)\u001b[49m\n",
            "\u001b[36mFile \u001b[39m\u001b[32m/opt/homebrew/Cellar/python@3.13/3.13.3_1/Frameworks/Python.framework/Versions/3.13/lib/python3.13/ssl.py:1285\u001b[39m, in \u001b[36mSSLSocket.recv\u001b[39m\u001b[34m(self, buflen, flags)\u001b[39m\n\u001b[32m   1281\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m flags != \u001b[32m0\u001b[39m:\n\u001b[32m   1282\u001b[39m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[32m   1283\u001b[39m             \u001b[33m\"\u001b[39m\u001b[33mnon-zero flags not allowed in calls to recv() on \u001b[39m\u001b[38;5;132;01m%s\u001b[39;00m\u001b[33m\"\u001b[39m %\n\u001b[32m   1284\u001b[39m             \u001b[38;5;28mself\u001b[39m.\u001b[34m__class__\u001b[39m)\n\u001b[32m-> \u001b[39m\u001b[32m1285\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mread\u001b[49m\u001b[43m(\u001b[49m\u001b[43mbuflen\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1286\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m   1287\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28msuper\u001b[39m().recv(buflen, flags)\n",
            "\u001b[36mFile \u001b[39m\u001b[32m/opt/homebrew/Cellar/python@3.13/3.13.3_1/Frameworks/Python.framework/Versions/3.13/lib/python3.13/ssl.py:1140\u001b[39m, in \u001b[36mSSLSocket.read\u001b[39m\u001b[34m(self, len, buffer)\u001b[39m\n\u001b[32m   1138\u001b[39m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m._sslobj.read(\u001b[38;5;28mlen\u001b[39m, buffer)\n\u001b[32m   1139\u001b[39m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m1140\u001b[39m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_sslobj\u001b[49m\u001b[43m.\u001b[49m\u001b[43mread\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mlen\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[32m   1141\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m SSLError \u001b[38;5;28;01mas\u001b[39;00m x:\n\u001b[32m   1142\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m x.args[\u001b[32m0\u001b[39m] == SSL_ERROR_EOF \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mself\u001b[39m.suppress_ragged_eofs:\n",
            "\u001b[31mKeyboardInterrupt\u001b[39m: "
          ]
        }
      ],
      "source": [
        "# Test the complete pipeline\n",
        "print(\"\\nTesting Multi-Node Pipeline:\")\n",
        "print(\"=\" * 60)\n",
        "\n",
        "# Initialize conversation state\n",
        "conversation_state = {\n",
        "    \"messages\": [],\n",
        "    \"summary\": \"\",\n",
        "    \"window_size\": 3\n",
        "}\n",
        "\n",
        "# Simulate a conversation\n",
        "test_messages = [\n",
        "    \"Hello! I'm interested in learning about LangGraph\",\n",
        "    \"What are the key features of LangGraph?\",\n",
        "    \"How does it compare to other frameworks?\",\n",
        "    \"Can you show me a simple example?\",\n",
        "    \"What about error handling?\"\n",
        "]\n",
        "\n",
        "for i, user_msg in enumerate(test_messages):\n",
        "    print(f\"\\n--- Turn {i+1} ---\")\n",
        "    print(f\"User: {user_msg}\")\n",
        "    \n",
        "    # Add user message to state\n",
        "    conversation_state[\"messages\"].append(HumanMessage(content=user_msg))\n",
        "    \n",
        "    # Process through pipeline\n",
        "    result = pipeline.invoke(conversation_state)\n",
        "    \n",
        "    # Update conversation state\n",
        "    conversation_state = result\n",
        "    \n",
        "    # Display the response\n",
        "    if result[\"messages\"]:\n",
        "        last_msg = result[\"messages\"][-1]\n",
        "        if isinstance(last_msg, AIMessage):\n",
        "            print(f\"Assistant: {last_msg.content[:150]}...\")\n",
        "    \n",
        "    print(f\"\\nState Info:\")\n",
        "    print(f\"  Messages in memory: {len(result['messages'])}\")\n",
        "    print(f\"  Summary length: {len(result['summary'])} chars\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Putting It All Together: Production-Ready Patterns\n",
        "\n",
        "Let's combine all the concepts into a production-ready conversational agent that demonstrates best practices for state management and multi-node processing.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 33,
      "metadata": {},
      "outputs": [],
      "source": [
        "class ProductionState(TypedDict):\n",
        "    \"\"\"\n",
        "    Production-ready state with comprehensive tracking.\n",
        "    \"\"\"\n",
        "    messages: Annotated[list[BaseMessage], add_messages]\n",
        "    summary: str\n",
        "    window_size: int\n",
        "    conversation_metadata: dict  # Track conversation metrics\n",
        "    user_context: dict  # Store user preferences/info"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 34,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Enhanced message processor with metadata tracking\n",
        "def enhanced_processor(state: ProductionState) -> ProductionState:\n",
        "        \"\"\"Process messages and update metadata.\"\"\"\n",
        "        metadata = state.get(\"conversation_metadata\", {})\n",
        "        metadata[\"total_messages\"] = metadata.get(\"total_messages\", 0) + 1\n",
        "        metadata[\"last_updated\"] = \"now\"\n",
        "        \n",
        "        # Extract user context from messages\n",
        "        user_context = state.get(\"user_context\", {})\n",
        "        if state[\"messages\"]:\n",
        "            last_msg = state[\"messages\"][-1]\n",
        "            if isinstance(last_msg, HumanMessage):\n",
        "                # Simple intent detection\n",
        "                content_lower = last_msg.content.lower()\n",
        "                if \"my name is\" in content_lower:\n",
        "                    # Extract name (simplified)\n",
        "                    words = last_msg.content.split()\n",
        "                    if \"is\" in words:\n",
        "                        idx = words.index(\"is\")\n",
        "                        if idx < len(words) - 1:\n",
        "                            user_context[\"name\"] = words[idx + 1].strip(\".,!?\")\n",
        "        \n",
        "        return {\n",
        "            **state,\n",
        "            \"conversation_metadata\": metadata,\n",
        "            \"user_context\": user_context\n",
        "        }"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 38,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Smart response generator using all context\n",
        "def smart_response_generator(state: ProductionState) -> ProductionState:\n",
        "        \"\"\"Generate responses using full context.\"\"\"\n",
        "        last_msg = state[\"messages\"][-1] if state[\"messages\"] else None\n",
        "        \n",
        "        if isinstance(last_msg, HumanMessage):\n",
        "            # Build comprehensive context\n",
        "            context_parts = [\n",
        "                f\"Summary: {state.get('summary', 'No summary yet')}\"\n",
        "            ]\n",
        "            \n",
        "            # Add user context if available\n",
        "            if state.get(\"user_context\", {}).get(\"name\"):\n",
        "                context_parts.append(f\"User name: {state['user_context']['name']}\")\n",
        "            \n",
        "            # Add conversation metadata\n",
        "            metadata = state.get(\"conversation_metadata\", {})\n",
        "            if metadata.get(\"total_messages\"):\n",
        "                context_parts.append(f\"Messages exchanged: {metadata['total_messages']}\")\n",
        "            \n",
        "            context_parts.append(f\"User message: {last_msg.content}\")\n",
        "            \n",
        "            full_context = \"\\n\".join(context_parts)\n",
        "            \n",
        "            prompt = PromptTemplate(\n",
        "                input_variables=[\"context\"],\n",
        "                template=\"\"\"You are a helpful assistant. Use all available context to provide personalized responses.\n",
        "                \n",
        "                Context:\n",
        "                {context}\n",
        "\n",
        "                Provide a helpful, personalized response:\"\"\"\n",
        "            )\n",
        "            \n",
        "            response = llm.invoke(prompt.format(context=full_context))\n",
        "            \n",
        "            return {\n",
        "                **state,\n",
        "                \"messages\": [AIMessage(content=response.content)]\n",
        "            }\n",
        "        \n",
        "        return state"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 39,
      "metadata": {},
      "outputs": [],
      "source": [
        "\n",
        "def create_production_agent():\n",
        "    \"\"\"\n",
        "    Create a production-ready conversational agent with all best practices.\n",
        "    \"\"\"\n",
        "    graph = StateGraph(ProductionState)\n",
        "    \n",
        "    # Add nodes\n",
        "    graph.add_node(\"enhanced_processor\", enhanced_processor)\n",
        "    graph.add_node(\"window_manager\", window_manager_node)\n",
        "    graph.add_node(\"summarizer\", summarizer_node)\n",
        "    graph.add_node(\"smart_responder\", smart_response_generator)\n",
        "    \n",
        "    # Define flow\n",
        "    graph.add_edge(START, \"enhanced_processor\")\n",
        "    graph.add_edge(\"enhanced_processor\", \"window_manager\")\n",
        "    graph.add_edge(\"window_manager\", \"summarizer\")\n",
        "    graph.add_edge(\"summarizer\", \"smart_responder\")\n",
        "    graph.add_edge(\"smart_responder\", END)\n",
        "    \n",
        "    return graph.compile()\n",
        "\n",
        "# Create production agent\n",
        "production_agent = create_production_agent()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 40,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "image/png": "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",
            "text/plain": [
              "<IPython.core.display.Image object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "# Visualize the pipeline structure\n",
        "from IPython.display import display, Image\n",
        "from langchain_core.runnables.graph import MermaidDrawMethod\n",
        "\n",
        "try:\n",
        "    display(\n",
        "        Image(\n",
        "            production_agent.get_graph().draw_mermaid_png(\n",
        "                draw_method=MermaidDrawMethod.API,\n",
        "            )\n",
        "        )\n",
        "    )\n",
        "except Exception as e:\n",
        "    print(f\"Visualization not available: {e}\")\n",
        "    print(\"\\nPipeline structure:\")\n",
        "    print(\"START -> processor -> window_manager -> summarizer -> response_generator -> END\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 41,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "Testing Production Agent:\n",
            "============================================================\n",
            "\n",
            "--- Turn 1 ---\n",
            "User: Hello! My name is Alex\n",
            "[Window Manager] Current messages: 1, Window: 4\n",
            "Assistant: Hello, Alex! It's great to meet you! How can I assist you today? If there's anything specific you'd like to talk about or any questions you have, feel free to share!...\n",
            "\n",
            "Enhanced State:\n",
            "  Window: 2/4\n",
            "  User Context: {'name': 'Alex'}\n",
            "  Metadata: {'total_messages': 1, 'last_updated': 'now'}\n",
            "\n",
            "--- Turn 2 ---\n",
            "User: I want to learn about building agents with LangGraph\n",
            "[Window Manager] Current messages: 3, Window: 4\n",
            "[Summarizer] Generating summary for 3 messages\n",
            "Assistant: Hi Alex! I'm excited to help you learn about building agents with LangGraph. To get started, it would be helpful to know what specific aspects you're interested in. Are you looking for an overview of ...\n",
            "\n",
            "Enhanced State:\n",
            "  Window: 4/4\n",
            "  User Context: {'name': 'Alex'}\n",
            "  Metadata: {'total_messages': 2, 'last_updated': 'now'}\n",
            "\n",
            "--- Turn 3 ---\n",
            "User: What makes it different from regular chatbots?\n",
            "[Window Manager] Current messages: 5, Window: 4\n",
            "[Summarizer] Generating summary for 5 messages\n",
            "Assistant: Hello, Alex! Great question! LangGraph differs from regular chatbots in several key ways:\n",
            "\n",
            "1. **Contextual Understanding**: LangGraph is designed to maintain a deeper understanding of context over lon...\n",
            "\n",
            "Enhanced State:\n",
            "  Window: 6/4\n",
            "  User Context: {'name': 'Alex'}\n",
            "  Metadata: {'total_messages': 3, 'last_updated': 'now'}\n",
            "\n",
            "--- Turn 4 ---\n",
            "User: Can you give me a specific example?\n",
            "[Window Manager] Current messages: 7, Window: 4\n",
            "[Summarizer] Generating summary for 7 messages\n",
            "Assistant: Of course, Alex! Let's dive into a specific example of building an agent using LangGraph.\n",
            "\n",
            "Imagine you want to create a simple chatbot agent that can answer questions about a specific topic, like \"hea...\n",
            "\n",
            "Enhanced State:\n",
            "  Window: 8/4\n",
            "  User Context: {'name': 'Alex'}\n",
            "  Metadata: {'total_messages': 4, 'last_updated': 'now'}\n",
            "\n",
            "--- Turn 5 ---\n",
            "User: Thanks, this is really helpful!\n",
            "[Window Manager] Current messages: 9, Window: 4\n",
            "[Summarizer] Generating summary for 9 messages\n",
            "Assistant: You're very welcome, Alex! I'm glad to hear that you found the information helpful. If you have any more questions about building agents with LangGraph or if there's a specific aspect you'd like to di...\n",
            "\n",
            "Enhanced State:\n",
            "  Window: 10/4\n",
            "  User Context: {'name': 'Alex'}\n",
            "  Metadata: {'total_messages': 5, 'last_updated': 'now'}\n"
          ]
        }
      ],
      "source": [
        "# Test the production agent\n",
        "print(\"\\nTesting Production Agent:\")\n",
        "print(\"=\" * 60)\n",
        "\n",
        "# Initialize state\n",
        "prod_state = {\n",
        "    \"messages\": [],\n",
        "    \"summary\": \"\",\n",
        "    \"window_size\": 4,\n",
        "    \"conversation_metadata\": {},\n",
        "    \"user_context\": {}\n",
        "}\n",
        "\n",
        "# Simulate a personalized conversation\n",
        "production_messages = [\n",
        "    \"Hello! My name is Alex\",\n",
        "    \"I want to learn about building agents with LangGraph\",\n",
        "    \"What makes it different from regular chatbots?\",\n",
        "    \"Can you give me a specific example?\",\n",
        "    \"Thanks, this is really helpful!\"\n",
        "]\n",
        "\n",
        "for i, msg in enumerate(production_messages):\n",
        "    print(f\"\\n--- Turn {i+1} ---\")\n",
        "    print(f\"User: {msg}\")\n",
        "    \n",
        "    # Add message and process\n",
        "    prod_state[\"messages\"].append(HumanMessage(content=msg))\n",
        "    result = production_agent.invoke(prod_state)\n",
        "    prod_state = result\n",
        "    \n",
        "    # Show response\n",
        "    if result[\"messages\"]:\n",
        "        last_msg = result[\"messages\"][-1]\n",
        "        if isinstance(last_msg, AIMessage):\n",
        "            print(f\"Assistant: {last_msg.content[:200]}...\")\n",
        "    \n",
        "    # Show enhanced state info\n",
        "    print(f\"\\nEnhanced State:\")\n",
        "    print(f\"  Window: {len(result['messages'])}/{result['window_size']}\")\n",
        "    print(f\"  User Context: {result.get('user_context', {})}\")\n",
        "    print(f\"  Metadata: {result.get('conversation_metadata', {})}\")\n"
      ]
    },
    {
      "cell_type": "raw",
      "metadata": {},
      "source": [
        "## Key Takeaways and Best Practices\n",
        "\n",
        "### 1. **Advanced State Management**\n",
        "- Use TypedDict for type-safe state definitions\n",
        "- Include fields beyond messages (summaries, metadata, user context)\n",
        "- Always preserve all state fields during updates\n",
        "- Initialize state fields with sensible defaults\n",
        "\n",
        "### 2. **Memory Optimization**\n",
        "- Implement sliding windows to control memory usage\n",
        "- Generate summaries before pruning messages\n",
        "- Balance window size with context needs\n",
        "- Consider different window sizes for different use cases\n",
        "\n",
        "### 3. **Multi-Node Architecture**\n",
        "- Separate concerns into specialized nodes\n",
        "- Make nodes composable and reusable\n",
        "- Use clear naming conventions\n",
        "- Document node responsibilities\n",
        "\n",
        "### 4. **Production Patterns**\n",
        "- Track conversation metadata for analytics\n",
        "- Store user context for personalization\n",
        "- Implement comprehensive error handling\n",
        "- Use proper logging for debugging\n",
        "\n",
        "### 5. **Common Pitfalls to Avoid**\n",
        "- ❌ Forgetting to preserve state fields\n",
        "- ❌ Not handling empty states\n",
        "- ❌ Ignoring memory constraints\n",
        "- ❌ Creating monolithic nodes\n",
        "- ❌ Missing error handling\n",
        "\n",
        "### 6. **Performance Considerations**\n",
        "- Optimize summary generation frequency\n",
        "- Cache frequently accessed data\n",
        "- Monitor state size growth\n",
        "- Profile node execution times\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "vscode": {
          "languageId": "raw"
        }
      },
      "source": [
        "## Next Steps\n",
        "\n",
        "Now that you've mastered advanced state management and multi-node processing:\n",
        "\n",
        "### 1. **Experiment with Different Architectures**\n",
        "- Try parallel node execution\n",
        "- Implement conditional routing\n",
        "- Create branching pipelines\n",
        "\n",
        "### 2. **Add Advanced Features**\n",
        "- Implement state persistence\n",
        "- Add conversation analytics\n",
        "- Create custom state validators\n",
        "\n",
        "### 3. **Optimize for Production**\n",
        "- Add comprehensive error handling\n",
        "- Implement retry logic\n",
        "- Create health checks\n",
        "\n",
        "### 4. **Explore Integration**\n",
        "- Connect to external APIs\n",
        "- Add tool usage capabilities\n",
        "- Implement multi-agent coordination\n",
        "\n",
        "### 5. **Study Real-World Examples**\n",
        "- Review the LangGraph documentation\n",
        "- Explore community examples\n",
        "- Build your own production system\n",
        "\n",
        "## Resources\n",
        "\n",
        "- [LangGraph Documentation](https://langchain-ai.github.io/langgraph/)\n",
        "- [AI Product Engineer Tutorials](https://aiproduct.engineer/tutorials/)\n",
        "- [LangChain Community](https://github.com/langchain-ai/langchain)\n",
        "\n",
        "Happy building! 🚀\n"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": ".venv",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.13.3"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 2
}
